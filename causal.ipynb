{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "accelerate==0.33.0\n",
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "certifi==2024.7.4\n",
      "charset-normalizer==3.3.2\n",
      "comm==0.2.2\n",
      "datasets==2.20.0\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "dill==0.3.8\n",
      "evaluate==0.4.2\n",
      "exceptiongroup==1.2.2\n",
      "executing==2.0.1\n",
      "filelock==3.13.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "huggingface-hub==0.24.2\n",
      "idna==3.7\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.3\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib-inline==0.1.7\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.2.1\n",
      "numpy==1.26.3\n",
      "nvidia-cublas-cu11==11.11.3.6\n",
      "nvidia-cuda-cupti-cu11==11.8.87\n",
      "nvidia-cuda-nvrtc-cu11==11.8.89\n",
      "nvidia-cuda-runtime-cu11==11.8.89\n",
      "nvidia-cudnn-cu11==9.1.0.70\n",
      "nvidia-cufft-cu11==10.9.0.58\n",
      "nvidia-curand-cu11==10.3.0.86\n",
      "nvidia-cusolver-cu11==11.4.1.48\n",
      "nvidia-cusparse-cu11==11.7.5.86\n",
      "nvidia-nccl-cu11==2.20.5\n",
      "nvidia-nvtx-cu11==11.8.86\n",
      "packaging==24.1\n",
      "pandas==2.2.2\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "pillow==10.2.0\n",
      "platformdirs==4.2.2\n",
      "prompt_toolkit==3.0.47\n",
      "psutil==6.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pyarrow==17.0.0\n",
      "pyarrow-hotfix==0.6\n",
      "Pygments==2.18.0\n",
      "python-dateutil==2.9.0.post0\n",
      "pytz==2024.1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "regex==2024.7.24\n",
      "requests==2.32.3\n",
      "safetensors==0.4.3\n",
      "six==1.16.0\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "tokenizers==0.19.1\n",
      "torch==2.4.0+cu118\n",
      "torchaudio==2.4.0+cu118\n",
      "torchvision==0.19.0+cu118\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.43.3\n",
      "triton==3.0.0\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.1\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n"
     ]
    }
   ],
   "source": [
    "! pip install -Uqq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install -Uqq transformers datasets evaluate accelerate \n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "! pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': '7g8a3o',\n",
       " 'title': 'Can you damage the inside of your body if you constantly eat hot food and drink hot beverages?',\n",
       " 'selftext': '',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['dqh8im1', 'dqh9r3t'],\n",
       " 'answers.text': [\"If the temperature of the food is high enough to burn your skin outside the body it will do so inside as well. Otherwise food cooling to body temperature isn't really a strain on the body.\",\n",
       "  \"Hot as in spicy food can cause distress in your GI tract, due to the chemicals irritating the tissues lining it. Otherwise, you should be fine as long as the food isn't hot enough to cause physical damage. If you experience persistent discomfort when eating certain foods, it might not be temperature that is the issue but some other form of irritation. A doctor would probably be able to assist you better.\"],\n",
       " 'answers.score': [6, 3],\n",
       " 'answers.text_urls': [[], []],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "eli5 = eli5.flatten().train_test_split(test_size=0.2)\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[40, 277, 220, 220, 256, 289, 304, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 267, 277, 220, 220, 256, 289, 304, 220, 220, 277, 267, 267, 288, 220, 220, 1312, 264, 220, 220, 289, 1312, 308, 289, 220, 220, 304, 299, 267, 334, 308, 289, 220, 220, 256, 267, 220, 220, 275, 334, 374, 299, 220, 220, 331, 267, 334, 374, 220, 220, 264, 479, 1312, 299, 220, 220, 267, 334, 256, 264, 1312, 288, 304, 220, 220, 256, 289, 304, 220, 220, 275, 267, 288, 331, 220, 220, 1312, 256, 220, 220, 266, 1312, 300, 300, 220, 220, 288, 267, 220, 220, 264, 267, 220, 220, 1312, 299, 264, 1312, 288, 304, 220, 220, 257, 264, 220, 220, 266, 304, 300, 300, 764, 220, 220, 440, 256, 289, 304, 374, 266, 1312, 264, 304, 220, 220, 277, 267, 267, 288, 220, 220, 269, 267, 267, 300, 1312, 299, 308, 220, 220, 256, 267, 220, 220, 275, 267, 288, 331, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 1312, 264, 299, 705, 256, 220, 220, 374, 304, 257, 300, 300, 331, 220, 220, 257, 220, 220, 264, 256, 374, 257, 1312, 299, 220, 220, 267, 299, 220, 220, 256, 289, 304, 220, 220, 275, 267, 288, 331, 764], [39, 267, 256, 220, 220, 257, 264, 220, 220, 1312, 299, 220, 220, 264, 279, 1312, 269, 331, 220, 220, 277, 267, 267, 288, 220, 220, 269, 257, 299, 220, 220, 269, 257, 334, 264, 304, 220, 220, 288, 1312, 264, 256, 374, 304, 264, 264, 220, 220, 1312, 299, 220, 220, 331, 267, 334, 374, 220, 220, 402, 314, 220, 220, 256, 374, 257, 269, 256, 837, 220, 220, 288, 334, 304, 220, 220, 256, 267, 220, 220, 256, 289, 304, 220, 220, 269, 289, 304, 285, 1312, 269, 257, 300, 264, 220, 220, 1312, 374, 374, 1312, 256, 257, 256, 1312, 299, 308, 220, 220, 256, 289, 304, 220, 220, 256, 1312, 264, 264, 334, 304, 264, 220, 220, 300, 1312, 299, 1312, 299, 308, 220, 220, 1312, 256, 764, 220, 220, 440, 256, 289, 304, 374, 266, 1312, 264, 304, 837, 220, 220, 331, 267, 334, 220, 220, 264, 289, 267, 334, 300, 288, 220, 220, 275, 304, 220, 220, 277, 1312, 299, 304, 220, 220, 257, 264, 220, 220, 300, 267, 299, 308, 220, 220, 257, 264, 220, 220, 256, 289, 304, 220, 220, 277, 267, 267, 288, 220, 220, 1312, 264, 299, 705, 256, 220, 220, 289, 267, 256, 220, 220, 304, 299, 267, 334, 308, 289, 220, 220, 256, 267, 220, 220, 269, 257, 334, 264, 304, 220, 220, 279, 289, 331, 264, 1312, 269, 257, 300, 220, 220, 288, 257, 285, 257, 308, 304, 764, 220, 220, 314, 277, 220, 220, 331, 267, 334, 220, 220, 304, 2124, 279, 304, 374, 1312, 304, 299, 269, 304, 220, 220, 279, 304, 374, 264, 1312, 264, 256, 304, 299, 256, 220, 220, 288, 1312, 264, 269, 267, 285, 277, 267, 374, 256, 220, 220, 266, 289, 304, 299, 220, 220, 304, 257, 256, 1312, 299, 308, 220, 220, 269, 304, 374, 256, 257, 1312, 299, 220, 220, 277, 267, 267, 288, 264, 837, 220, 220, 1312, 256, 220, 220, 285, 1312, 308, 289, 256, 220, 220, 299, 267, 256, 220, 220, 275, 304, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 256, 289, 257, 256, 220, 220, 1312, 264, 220, 220, 256, 289, 304, 220, 220, 1312, 264, 264, 334, 304, 220, 220, 275, 334, 256, 220, 220, 264, 267, 285, 304, 220, 220, 267, 256, 289, 304, 374, 220, 220, 277, 267, 374, 285, 220, 220, 267, 277, 220, 220, 1312, 374, 374, 1312, 256, 257, 256, 1312, 267, 299, 764, 220, 220, 317, 220, 220, 288, 267, 269, 256, 267, 374, 220, 220, 266, 267, 334, 300, 288, 220, 220, 279, 374, 267, 275, 257, 275, 300, 331, 220, 220, 275, 304, 220, 220, 257, 275, 300, 304, 220, 220, 256, 267, 220, 220, 257, 264, 264, 1312, 264, 256, 220, 220, 331, 267, 334, 220, 220, 275, 304, 256, 256, 304, 374, 764]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "preprocess_function(eli5[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4076 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  25%|██▌       | 1000/4000 [00:00<00:01, 1614.35 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 4000/4000 [00:00<00:00, 4952.98 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1282 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1359 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1385 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:00, 1343.58 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1295 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 3479.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4000/4000 [00:02<00:00, 1848.19 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 4463.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3882' max='3882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3882/3882 07:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.952400</td>\n",
       "      <td>3.824386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.855600</td>\n",
       "      <td>3.814416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.819900</td>\n",
       "      <td>3.813229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3882, training_loss=3.8807293804941074, metrics={'train_runtime': 470.4882, 'train_samples_per_second': 65.995, 'train_steps_per_second': 8.251, 'total_flos': 1014158013235200.0, 'train_loss': 3.8807293804941074, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"causal\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "tokenizer.save_pretrained(\"causal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 45.30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Somatic hypermutation allows the immune system to be effectively weakened due to the different genetic predisposition that predisposes people to the disease. There is also genetic variation that can change your blood\\'s sensitivity to certain types of virus. This is called \"reverse transcription of a virus\". It\\'s called epigenetic transcription. When DNA is broken down, it\\'s changed in the form of epigenetic variants (e.g. epigenetic mutations). This is where the genes come from (e.g. epigenetic variants). Some genes have a much higher']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,  AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"causal\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"causal/checkpoint-3882\")\n",
    "\n",
    "prompt = \"Somatic hypermutation allows the immune system to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
