{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "accelerate==0.33.0\n",
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "asttokens==2.4.1\n",
      "async-timeout==4.0.3\n",
      "attrs==23.2.0\n",
      "certifi==2024.7.4\n",
      "charset-normalizer==3.3.2\n",
      "comm==0.2.2\n",
      "datasets==2.20.0\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "dill==0.3.8\n",
      "evaluate==0.4.2\n",
      "exceptiongroup==1.2.2\n",
      "executing==2.0.1\n",
      "filelock==3.13.1\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.2.0\n",
      "huggingface-hub==0.24.2\n",
      "idna==3.7\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.3\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib-inline==0.1.7\n",
      "mpmath==1.3.0\n",
      "multidict==6.0.5\n",
      "multiprocess==0.70.16\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.2.1\n",
      "numpy==1.26.3\n",
      "nvidia-cublas-cu11==11.11.3.6\n",
      "nvidia-cuda-cupti-cu11==11.8.87\n",
      "nvidia-cuda-nvrtc-cu11==11.8.89\n",
      "nvidia-cuda-runtime-cu11==11.8.89\n",
      "nvidia-cudnn-cu11==9.1.0.70\n",
      "nvidia-cufft-cu11==10.9.0.58\n",
      "nvidia-curand-cu11==10.3.0.86\n",
      "nvidia-cusolver-cu11==11.4.1.48\n",
      "nvidia-cusparse-cu11==11.7.5.86\n",
      "nvidia-nccl-cu11==2.20.5\n",
      "nvidia-nvtx-cu11==11.8.86\n",
      "packaging==24.1\n",
      "pandas==2.2.2\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "pillow==10.2.0\n",
      "platformdirs==4.2.2\n",
      "prompt_toolkit==3.0.47\n",
      "psutil==6.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure_eval==0.2.3\n",
      "pyarrow==17.0.0\n",
      "pyarrow-hotfix==0.6\n",
      "Pygments==2.18.0\n",
      "python-dateutil==2.9.0.post0\n",
      "pytz==2024.1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "regex==2024.7.24\n",
      "requests==2.32.3\n",
      "safetensors==0.4.3\n",
      "six==1.16.0\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "tokenizers==0.19.1\n",
      "torch==2.4.0+cu118\n",
      "torchaudio==2.4.0+cu118\n",
      "torchvision==0.19.0+cu118\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.43.3\n",
      "triton==3.0.0\n",
      "typing_extensions==4.12.2\n",
      "tzdata==2024.1\n",
      "ujson==5.10.0\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "xxhash==3.4.1\n",
      "yarl==1.9.4\n"
     ]
    }
   ],
   "source": [
    "! pip install -Uqq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install -Uqq transformers datasets evaluate accelerate ujson\n",
    "import torch\n",
    "print(f\"CUDA={torch.cuda.is_available()}\")\n",
    "! pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUSAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'q_id': '7g8a3o',\n",
       " 'title': 'Can you damage the inside of your body if you constantly eat hot food and drink hot beverages?',\n",
       " 'selftext': '',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['dqh8im1', 'dqh9r3t'],\n",
       " 'answers.text': [\"If the temperature of the food is high enough to burn your skin outside the body it will do so inside as well. Otherwise food cooling to body temperature isn't really a strain on the body.\",\n",
       "  \"Hot as in spicy food can cause distress in your GI tract, due to the chemicals irritating the tissues lining it. Otherwise, you should be fine as long as the food isn't hot enough to cause physical damage. If you experience persistent discomfort when eating certain foods, it might not be temperature that is the issue but some other form of irritation. A doctor would probably be able to assist you better.\"],\n",
       " 'answers.score': [6, 3],\n",
       " 'answers.text_urls': [[], []],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n",
    "eli5 = eli5.flatten().train_test_split(test_size=0.2)\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[40, 277, 220, 220, 256, 289, 304, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 267, 277, 220, 220, 256, 289, 304, 220, 220, 277, 267, 267, 288, 220, 220, 1312, 264, 220, 220, 289, 1312, 308, 289, 220, 220, 304, 299, 267, 334, 308, 289, 220, 220, 256, 267, 220, 220, 275, 334, 374, 299, 220, 220, 331, 267, 334, 374, 220, 220, 264, 479, 1312, 299, 220, 220, 267, 334, 256, 264, 1312, 288, 304, 220, 220, 256, 289, 304, 220, 220, 275, 267, 288, 331, 220, 220, 1312, 256, 220, 220, 266, 1312, 300, 300, 220, 220, 288, 267, 220, 220, 264, 267, 220, 220, 1312, 299, 264, 1312, 288, 304, 220, 220, 257, 264, 220, 220, 266, 304, 300, 300, 764, 220, 220, 440, 256, 289, 304, 374, 266, 1312, 264, 304, 220, 220, 277, 267, 267, 288, 220, 220, 269, 267, 267, 300, 1312, 299, 308, 220, 220, 256, 267, 220, 220, 275, 267, 288, 331, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 1312, 264, 299, 705, 256, 220, 220, 374, 304, 257, 300, 300, 331, 220, 220, 257, 220, 220, 264, 256, 374, 257, 1312, 299, 220, 220, 267, 299, 220, 220, 256, 289, 304, 220, 220, 275, 267, 288, 331, 764], [39, 267, 256, 220, 220, 257, 264, 220, 220, 1312, 299, 220, 220, 264, 279, 1312, 269, 331, 220, 220, 277, 267, 267, 288, 220, 220, 269, 257, 299, 220, 220, 269, 257, 334, 264, 304, 220, 220, 288, 1312, 264, 256, 374, 304, 264, 264, 220, 220, 1312, 299, 220, 220, 331, 267, 334, 374, 220, 220, 402, 314, 220, 220, 256, 374, 257, 269, 256, 837, 220, 220, 288, 334, 304, 220, 220, 256, 267, 220, 220, 256, 289, 304, 220, 220, 269, 289, 304, 285, 1312, 269, 257, 300, 264, 220, 220, 1312, 374, 374, 1312, 256, 257, 256, 1312, 299, 308, 220, 220, 256, 289, 304, 220, 220, 256, 1312, 264, 264, 334, 304, 264, 220, 220, 300, 1312, 299, 1312, 299, 308, 220, 220, 1312, 256, 764, 220, 220, 440, 256, 289, 304, 374, 266, 1312, 264, 304, 837, 220, 220, 331, 267, 334, 220, 220, 264, 289, 267, 334, 300, 288, 220, 220, 275, 304, 220, 220, 277, 1312, 299, 304, 220, 220, 257, 264, 220, 220, 300, 267, 299, 308, 220, 220, 257, 264, 220, 220, 256, 289, 304, 220, 220, 277, 267, 267, 288, 220, 220, 1312, 264, 299, 705, 256, 220, 220, 289, 267, 256, 220, 220, 304, 299, 267, 334, 308, 289, 220, 220, 256, 267, 220, 220, 269, 257, 334, 264, 304, 220, 220, 279, 289, 331, 264, 1312, 269, 257, 300, 220, 220, 288, 257, 285, 257, 308, 304, 764, 220, 220, 314, 277, 220, 220, 331, 267, 334, 220, 220, 304, 2124, 279, 304, 374, 1312, 304, 299, 269, 304, 220, 220, 279, 304, 374, 264, 1312, 264, 256, 304, 299, 256, 220, 220, 288, 1312, 264, 269, 267, 285, 277, 267, 374, 256, 220, 220, 266, 289, 304, 299, 220, 220, 304, 257, 256, 1312, 299, 308, 220, 220, 269, 304, 374, 256, 257, 1312, 299, 220, 220, 277, 267, 267, 288, 264, 837, 220, 220, 1312, 256, 220, 220, 285, 1312, 308, 289, 256, 220, 220, 299, 267, 256, 220, 220, 275, 304, 220, 220, 256, 304, 285, 279, 304, 374, 257, 256, 334, 374, 304, 220, 220, 256, 289, 257, 256, 220, 220, 1312, 264, 220, 220, 256, 289, 304, 220, 220, 1312, 264, 264, 334, 304, 220, 220, 275, 334, 256, 220, 220, 264, 267, 285, 304, 220, 220, 267, 256, 289, 304, 374, 220, 220, 277, 267, 374, 285, 220, 220, 267, 277, 220, 220, 1312, 374, 374, 1312, 256, 257, 256, 1312, 267, 299, 764, 220, 220, 317, 220, 220, 288, 267, 269, 256, 267, 374, 220, 220, 266, 267, 334, 300, 288, 220, 220, 279, 374, 267, 275, 257, 275, 300, 331, 220, 220, 275, 304, 220, 220, 257, 275, 300, 304, 220, 220, 256, 267, 220, 220, 257, 264, 264, 1312, 264, 256, 220, 220, 331, 267, 334, 220, 220, 275, 304, 256, 256, 304, 374, 764]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n",
    "\n",
    "preprocess_function(eli5[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4076 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1376 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  25%|██▌       | 1000/4000 [00:00<00:01, 1614.35 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 4000/4000 [00:00<00:00, 4952.98 examples/s]\n",
      "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1282 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1359 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1385 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4):  25%|██▌       | 250/1000 [00:00<00:00, 1343.58 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1295 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 3479.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 4000/4000 [00:02<00:00, 1848.19 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 4463.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3882' max='3882' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3882/3882 07:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.952400</td>\n",
       "      <td>3.824386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.855600</td>\n",
       "      <td>3.814416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.819900</td>\n",
       "      <td>3.813229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3882, training_loss=3.8807293804941074, metrics={'train_runtime': 470.4882, 'train_samples_per_second': 65.995, 'train_steps_per_second': 8.251, 'total_flos': 1014158013235200.0, 'train_loss': 3.8807293804941074, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"causal\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "tokenizer.save_pretrained(\"causal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='342' max='342' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [342/342 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 45.30\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Somatic hypermutation allows the immune system to be effectively weakened due to the different genetic predisposition that predisposes people to the disease. There is also genetic variation that can change your blood\\'s sensitivity to certain types of virus. This is called \"reverse transcription of a virus\". It\\'s called epigenetic transcription. When DNA is broken down, it\\'s changed in the form of epigenetic variants (e.g. epigenetic mutations). This is where the genes come from (e.g. epigenetic variants). Some genes have a much higher']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM,  AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"causal\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"causal/checkpoint-3882\")\n",
    "\n",
    "prompt = \"Somatic hypermutation allows the immune system to\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANTARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'هَل غادَرَ الشُعَراءُ مِن مُتَرَدَّمِ ∴ أَم هَل عَرَفتَ الدارَ بَعدَ تَوَهُّمِ\\nيا دارَ عَبلَةَ بِالجَواءِ تَكَلَّمي ∴ وَعَمي صَباحاً دارَ عَبلَةَ وَاِسلَمي\\nفَوَقَفتُ فيها ناقَتي وَكَأَنَّه ∴ فَدَنٌ لِأَقضِيَ حاجَةَ المُتَلَوِّمِ\\nوَتَحُلُّ عَبلَةُ بِالجَواءِ وَأَهلُن ∴ بِالحَزنِ فَالصَمّانِ فَالمُتَثَلَّمِ\\nحُيِّيتَ مِن طَلَلٍ تَقادَمَ عَهدُهُ ∴ أَقوى وَأَقفَرَ بَعدَ أُمِّ الهَيثَمِ\\nحَلَّت بِأَرضِ الزائِرينَ فَأَصبَحَت ∴ عَسِراً عَلَيَّ طِلابُكِ اِبنَةَ مَخرَمِ\\nعُلِّقتُها عَرَضاً وَأَقتُلُ قَومَه ∴ زَعماً لَعَمرُ أَبيكَ لَيسَ بِمَزعَمِ\\nوَلَقَد نَزَلتِ فَلا تَظُنّي غَيرَهُ ∴ مِنّي بِمَنزِلَةِ المُحَبِّ المُكرَمِ\\nكَيفَ المَزارُ وَقَد تَرَبَّعَ أَهلُه ∴ بِعُنَيزَتَينِ وَأَهلُنا بِالغَيلَمِ\\nإِن كُنتِ أَزمَعتِ الفِراقَ فَإِنَّم ∴ زُمَّت رِكابُكُمُ بِلَيلٍ مُظلِمِ\\nما راعَني إِلّا حَمولَةُ أَهلِه ∴ وَسطَ الدِيارِ تَسَفُّ حَبَّ الخِمخِمِ\\nفيها اِثنَتانِ وَأَربَعونَ حَلوبَةً ∴ سوداً كَخافِيَةِ الغُرابِ الأَسحَمِ\\nإِذ تَستَبيكَ بِذي غُروبٍ واضِحٍ ∴ عَذبٍ مُقَبَّلُهُ لَذيذِ المَطعَمِ\\nوَكَأَنَّ فارَةَ تاجِرٍ بِقَسيمَةٍ ∴ سَبَقَت عَوارِضَها إِلَيكَ مِنَ الفَمِ\\nأَو رَوضَةً أُنُفاً تَضَمَّنَ نَبتَه ∴ غَيثٌ قَليلُ الدِمنِ لَيسَ بِمَعلَمِ\\nجادَت عَليهِ كُلُّ بِكرٍ حُرَّةٍ ∴ فَتَرَكنَ كُلَّ قَرارَةٍ كَالدِرهَمِ\\nسَحّاً وَتَسكاباً فَكُلَّ عَشِيَّةٍ ∴ يَجري عَلَيها الماءُ لَم يَتَصَرَّمِ\\nوَخَلا الذُبابُ بِها فَلَيسَ بِبارِحٍ ∴ غَرِداً كَفِعلِ الشارِبِ المُتَرَنِّمِ\\nهَزِجاً يَحُكُّ ذِراعَهُ بِذِراعِهِ ∴ قَدحَ المُكِبِّ عَلى الزِنادِ الأَجذَمِ\\nتُمسي وَتُصبِحُ فَوقَ ظَهرِ حَشِيَّةٍ ∴ وَأَبيتُ فَوقَ سَراةِ أَدهَمَ مُلجَمِ\\nوَحَشِيَّتي سَرجٌ عَلى عَبلِ الشَوى ∴ نَهدٍ مَراكِلُهُ نَبيلِ المَحزِمِ\\nهَل تُبلِغَنّي دارَها شَدَنِيَّةٌ ∴ لُعِنَت بِمَحرومِ الشَرابِ مُصَرَّمِ\\nخَطّارَةٌ غِبَّ السُرى زَيّافَةٌ ∴ تَطِسُ الإِكامَ بِوَخذِ خُفٍّ ميثَمِ\\nوَكَأَنَّما تَطِسُ الإِكامَ عَشِيَّةً ∴ بِقَريبِ بَينَ المَنسِمَينِ مُصَلَّمِ\\nتَأوي لَهُ قُلُصُ النَعامِ كَما أَوَت ∴ حِزَقٌ يَمانِيَةٌ لِأَعجَمَ طِمطِمِ\\nيَتبَعنَ قُلَّةَ رَأسِهِ وَكَأَنَّهُ ∴ حِدجٌ عَلى نَعشٍ لَهُنَّ مُخَيَّمِ\\nصَعلٍ يَعودُ بِذي العُشَيرَةِ بَيضَهُ ∴ كَالعَبدِ ذي الفَروِ الطَويلِ الأَصلَمِ\\nشَرِبَت بِماءِ الدُحرُضَينِ فَأَصبَحَت ∴ زَوراءَ تَنفِرُ عَن حِياضِ الدَيلَمِ\\nوَكَأَنَّما تَنأى بِجانِبِ دَفَّها ال ∴ وَحشِيِّ مِن هَزِجِ العَشِيِّ مُؤَوَّمِ\\nهِرٍ جَنيبٍ كُلَّما عَطَفَت لَهُ ∴ غَضَبى اِتَّقاها بِاليَدَينِ وَبِالفَمِ\\nبَرَكَت عَلى جَنبِ الرِداعِ كَأَنَّم ∴ بَرَكَت عَلى قَصَبٍ أَجَشَّ مُهَضَّمِ\\nوَكأَنَّ رُبّاً أَو كُحَيلاً مُعقَد ∴ حَشَّ الوَقودُ بِهِ جَوانِبَ قُمقُمِ\\nيَنباعُ مِن ذِفرى غَضوبٍ جَسرَةٍ ∴ زَيّافَةٍ مِثلَ الفَنيقِ المُكدَمِ\\nإِن تُغدِفي دوني القِناعَ فَإِنَّني ∴ طَبٌّ بِأَخذِ الفارِسِ المُستَلئِمِ\\nأَثني عَلَيَّ بِما عَلِمتِ فَإِنَّني ∴ سَمحٌ مُخالَقَتي إِذا لَم أُظلَمِ\\nوَإِذا ظُلِمتُ فَإِنَّ ظُلمِيَ باسِلٌ ∴ مُرٌّ مَذاقَتَهُ كَطَعمِ العَلقَمِ\\nوَلَقَد شَرِبتُ مِنَ المُدامَةِ بَعدَم ∴ رَكَدَ الهَواجِرُ بِالمَشوفِ المُعلَمِ\\nبِزُجاجَةٍ صَفراءَ ذاتِ أَسِرَّةٍ ∴ قُرِنَت بِأَزهَرَ في الشَمالِ مُفَدَّمِ\\nفَإِذا شَرِبتُ فَإِنَّني مُستَهلِكٌ ∴ مالي وَعِرضي وافِرٌ لَم يُكلَمِ\\nوَإِذا صَحَوتُ فَما أُقَصِّرُ عَن نَدىً ∴ وَكَما عَلِمتِ شَمائِلي وَتَكَرُّمي\\nوَحَليلِ غانِيَةٍ تَرَكتُ مُجَدَّل ∴ تَمكو فَريصَتُهُ كَشَدقِ الأَعلَمِ\\nسَبَقَت يَدايَ لَهُ بِعاجِلِ طَعنَةٍ ∴ وَرَشاشِ نافِذَةٍ كَلَونِ العَندَمِ\\nهَلّا سَأَلتِ الخَيلَ يا اِبنَةَ مالِكٍ ∴ إِن كُنتِ جاهِلَةً بِما لَم تَعلَمي\\nإِذ لا أَزالُ عَلى رِحالَةِ سابِحٍ ∴ نَهدٍ تَعاوَرُهُ الكُماةُ مُكَلَّمِ\\nطَوراً يُجَرَّدُ لِلطِعانِ وَتارَةً ∴ يَأوي إِلى حَصدِ القَسِيِّ عَرَمرَمِ\\nيُخبِركِ مَن شَهِدَ الوَقيعَةَ أَنَّني ∴ أَغشى الوَغى وَأَعِفُّ عِندَ المَغنَمِ\\nوَمُدَجَّجٍ كَرِهَ الكُماةُ نِزالَهُ ∴ لا مُمعِنٍ هَرَباً وَلا مُستَسلِمِ\\nجادَت لَهُ كَفّي بِعاجِلِ طَعنَةٍ ∴ بِمُثَقَّفٍ صَدقِ الكُعوبِ مُقَوَّمِ\\nفَشَكَكتُ بِالرُمحِ الأَصَمِّ ثِيابَهُ ∴ لَيسَ الكَريمُ عَلى القَنا بِمُحَرَّمِ\\nفَتَرَكتُهُ جَزَرَ السِباعِ يَنُشنَهُ ∴ يَقضِمنَ حُسنَ بِنانِهِ وَالمِعصَمِ\\nوَمِشَكِّ سابِغَةٍ هَتَكتُ فُروجَه ∴ بِالسَيفِ عَن حامي الحَقيقَةِ مُعلِمِ\\nرَبِذٍ يَداهُ بِالقِداحِ إِذا شَت ∴ هَتّاكِ غاياتِ التِجارِ مُلَوَّمِ\\nلَمّا رَآني قَد نَزَلتُ أُريدُهُ ∴ أَبدى نَواجِذَهُ لِغَيرِ تَبَسُّمِ\\nعَهدي بِهِ مَدَّ النَهارِ كَأَنَّم ∴ خُضِبَ البَنانُ وَرَأسُهُ بِالعِظلِمِ\\nفَطَعَنتُهُ بِالرُمحِ ثُمَّ عَلَوتُهُ ∴ بِمُهَنَّدٍ صافي الحَديدَةِ مِخذَمِ\\nبَطَلٍ كَأَنَّ ثِيابَهُ في سَرحَةٍ ∴ يُحذى نِعالَ السِبتِ لَيسَ بِتَوأَمِ\\nيا شاةَ ما قَنَصٍ لِمَن حَلَّت لَهُ ∴ حَرُمَت عَلَيَّ وَلَيتَها لَم تَحرُمِ\\nفَبَعَثتُ جارِيَتي فَقُلتُ لَها اِذهَبي ∴ فَتَجَسَّسي أَخبارَها لِيَ وَاِعلَمي\\nقالَت رَأَيتُ مِنَ الأَعادي غِرَّةً ∴ وَالشاةُ مُمكِنَةٌ لِمَن هُوَ مُرتَمِ\\nوَكَأَنَّما اِلتَفَتَت بِجيدِ جَدايَةٍ ∴ رَشإٍ مِنَ الغِزلانِ حُرٍّ أَرثَمِ\\nنِبِّئتُ عَمرواً غَيرَ شاكِرِ نِعمَتي ∴ وَالكُفرُ مَخبَثَةٌ لَنَفسِ المُنعِمِ\\nوَلَقَد حَفِظتُ وَصاةَ عَمّي بِالضُحى ∴ إِذ تَقلِصُ الشَفَتانِ عَن وَضَحِ الفَمِ\\nفي حَومَةِ الحَربِ الَّتي لا تَشتَكي ∴ غَمَراتِها الأَبطالُ غَيرَ تَغَمغُمِ\\nإِذ يَتَّقونَ بِيَ الأَسِنَّةَ لَم أَخِم ∴ عَنها وَلَكِنّي تَضايَقَ مُقدَمي\\nلَمّا رَأَيتُ القَومَ أَقبَلَ جَمعُهُم ∴ يَتَذامَرونَ كَرَرتُ غَيرَ مُذَمَّمِ\\nيَدعونَ عَنتَرَ وَالرِماحُ كَأَنَّه ∴ أَشطانُ بِئرٍ في لَبانِ الأَدهَمِ\\nما زِلتُ أَرميهِم بِثُغرَةِ نَحرِهِ ∴ وَلَبانِهِ حَتّى تَسَربَلَ بِالدَمِ\\nفَاِزوَرَّ مِن وَقعِ القَنا بِلَبانِهِ ∴ وَشَكا إِلَيَّ بِعَبرَةٍ وَتَحَمحُمِ\\nلَو كانَ يَدري ما المُحاوَرَةُ اِشتَكى ∴ وَلَكانَ لَو عَلِمَ الكَلامَ مُكَلِّمي\\nوَلَقَد شَفى نَفسي وَأَذهَبَ سُقمَه ∴ قيلُ الفَوارِسِ وَيكَ عَنتَرَ أَقدِمِ\\nوَالخَيلُ تَقتَحِمُ الخَبارَ عَوابِس ∴ مِن بَينِ شَيظَمَةٍ وَآخَرَ شَيظَمِ\\nذُلُلٌ رِكابي حَيثُ شِئتُ مُشايِعي ∴ لُبّي وَأَحفِزُهُ بِأَمرٍ مُبرَمِ\\nوَلَقَد خَشيتُ بِأَن أَموتَ وَلَم تَدُر ∴ لِلحَربِ دائِرَةٌ عَلى اِبنَي ضَمضَمِ\\nالشاتِمَي عِرضي وَلَم أَشتِمهُم ∴ وَالناذِرَينِ إِذا لَم اَلقَهُما دَمي\\nإِن يَفعَلا فَلَقَد تَرَكتُ أَباهُم ∴ جَزَرَ السِباعِ وَكُلِّ نَسرٍ قَشعَمِ'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ujson\n",
    "\n",
    "data = []\n",
    "with open(\"aldiwan.json\") as file:\n",
    "    for poet in ujson.load(file):\n",
    "        for poem in poet['poems']:\n",
    "            for i, half in enumerate(poem['halves']):\n",
    "                if i == 0:\n",
    "                    poem_concat = []\n",
    "                elif i % 2 == 1:\n",
    "                    poem_concat.append(' ∴ ')\n",
    "                else:\n",
    "                    poem_concat.append('\\n')\n",
    "                poem_concat.append(half)\n",
    "            data.append(''.join(poem_concat))\n",
    "\n",
    "print(len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'الصادعون صَفَا من لا يواجدهم ∴ والمُرْأبون بأذن الله ما شعَبوا'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "data_list = [{'text': d}  for d in data][:10000]\n",
    "train_test = Dataset.from_list(data_list).train_test_split(test_size=.5)\n",
    "test_valid = train_test['test'].train_test_split(test_size=.5)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_valid['train'],\n",
    "    'valid': test_valid['test']})\n",
    "print(dataset['train'][0])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"aubmindlab/aragpt2-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[271], [153, 230], [311], [271], [277], [276], [262], [263], [224], [311], [153, 240], [279], [153, 240], [271], [224], [153, 231], [263], [224], [153, 230], [271], [224], [153, 236], [262], [271], [299], [277], [280], [153, 231], [224], [162, 234, 116], [224], [262], [271], [153, 230], [153, 231], [153, 241], [269], [153, 244], [349], [275], [262], [263], [224], [275], [349], [333], [263], [224], [271], [153, 230], [153, 230], [280], [224], [153, 231], [271], [224], [313], [276], [153, 240], [275], [262], [271]], 'attention_mask': [[1], [1, 1], [1], [1], [1], [1], [1], [1], [1], [1], [1, 1], [1], [1, 1], [1], [1], [1, 1], [1], [1], [1, 1], [1], [1], [1, 1], [1], [1], [1], [1], [1], [1, 1], [1], [1, 1, 1], [1], [1], [1], [1, 1], [1, 1], [1, 1], [1], [1, 1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1, 1], [1, 1], [1], [1], [1, 1], [1], [1], [1], [1], [1, 1], [1], [1], [1]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"text\"]])\n",
    "\n",
    "preprocess_function(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:02<00:00, 2095.38 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2500/2500 [00:01<00:00, 1736.27 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2500/2500 [00:01<00:00, 1852.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5000/5000 [00:11<00:00, 441.05 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2500/2500 [00:04<00:00, 566.21 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 2500/2500 [00:04<00:00, 589.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17595' max='17595' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17595/17595 1:09:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.036300</td>\n",
       "      <td>0.978823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.969600</td>\n",
       "      <td>0.929720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.915054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gantara/tokenizer_config.json',\n",
       " 'gantara/special_tokens_map.json',\n",
       " 'gantara/vocab.json',\n",
       " 'gantara/merges.txt',\n",
       " 'gantara/added_tokens.json',\n",
       " 'gantara/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gantara\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()\n",
    "tokenizer.save_pretrained(\"gantara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "gantara/checkpoint-20000 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/gantara/checkpoint-20000/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66a74f8a-69a7210d682a06e06eade49a;1f8117e0-1aa6-47bc-a312-18c84c0c5dcb)\n\nRepository Not Found for url: https://huggingface.co/gantara/checkpoint-20000/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mهل غادر الشعراء من متردم\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgantara\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgantara/checkpoint-20000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m generator(prompt)\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:768\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: gantara/checkpoint-20000 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "prompt = \"هل غادر الشعراء من متردم\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gantara\")\n",
    "generator = pipeline(\"text-generation\",\n",
    "                     model=\"gantara/checkpoint-20000\", tokenizer=tokenizer)\n",
    "generator(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
