{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/msid/fastai/venv/lib/python3.10/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqq torch torchtext torchdata\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from time import monotonic\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.spatial.distance import cosine\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import WikiText103\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Word2VecParams:\n",
    "\n",
    "    # skipgram parameters\n",
    "    MIN_FREQ = 50 \n",
    "    SKIPGRAM_N_WORDS = 8\n",
    "    T = 85\n",
    "    NEG_SAMPLES = 50\n",
    "    NS_ARRAY_LEN = 5_000_000\n",
    "    SPECIALS = \"<unk>\"\n",
    "    TOKENIZER = 'basic_english'\n",
    "\n",
    "    # network parameters\n",
    "    BATCH_SIZE = 512\n",
    "    EMBED_DIM = 128\n",
    "    EMBED_MAX_NORM = None\n",
    "    N_EPOCHS = 5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    CRITERION = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def get_data():\n",
    "    def corpus_from_file(path):\n",
    "        with open(path, 'r') as file:\n",
    "            return file.readlines()\n",
    "    return corpus_from_file('datasets/wikitext2/train.csv'), corpus_from_file('datasets/wikitext2/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, list, specials):\n",
    "        self.stoi = {v[0]:(k, v[1]) for k, v in enumerate(list)}\n",
    "        self.itos = {k:(v[0], v[1]) for k, v in enumerate(list)}\n",
    "        self._specials = specials[0]\n",
    "        self.total_tokens = np.nansum(\n",
    "            [f for _, (_, f) in self.stoi.items()]\n",
    "            , dtype=int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi) - 1\n",
    "\n",
    "    def get_index(self, word: Union[str, List]):\n",
    "        if isinstance(word, str):\n",
    "            if word in self.stoi: \n",
    "                return self.stoi.get(word)[0]\n",
    "            else:\n",
    "                return self.stoi.get(self._specials)[0]\n",
    "        elif isinstance(word, list):\n",
    "            res = []\n",
    "            for w in word:\n",
    "                if w in self.stoi: \n",
    "                    res.append(self.stoi.get(w)[0])\n",
    "                else:\n",
    "                    res.append(self.stoi.get(self._specials)[0])\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Word {word} is not a string or a list of strings.\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def get_freq(self, word: Union[str, List]):\n",
    "        if isinstance(word, str):\n",
    "            if word in self.stoi: \n",
    "                return self.stoi.get(word)[1]\n",
    "            else:\n",
    "                return self.stoi.get(self._specials)[1]\n",
    "        elif isinstance(word, list):\n",
    "            res = []\n",
    "            for w in word:\n",
    "                if w in self.stoi:\n",
    "                    res.append(self.stoi.get(w)[1])\n",
    "                else:\n",
    "                    res.append(self.stoi.get(self._specials)[1])\n",
    "            return res\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Word {word} is not a string or a list of strings.\"\n",
    "                )\n",
    "    \n",
    "\n",
    "    def lookup_token(self, token: Union[int, List]):\n",
    "        if isinstance(token, (int, np.int64)):\n",
    "            if token in self.itos:\n",
    "                return self.itos.get(token)[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Token {token} not in vocabulary\")\n",
    "        elif isinstance(token, list):\n",
    "            res = []\n",
    "            for t in token:\n",
    "                if t in self.itos:\n",
    "                    res.append(self.itos.get(token)[0])\n",
    "                else:\n",
    "                    raise ValueError(f\"Token {t} is not a valid index.\")\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(iterator, tokenizer):\n",
    "    r = re.compile('[a-z1-9]')\n",
    "    for text in iterator:\n",
    "        res = tokenizer(text)\n",
    "        res = list(filter(r.match, res))\n",
    "        yield res\n",
    "\n",
    "def _build_vocab(ordered_dict: Dict, min_freq: int = 1, specials: str = '<unk>'):\n",
    "    tokens = []\n",
    "    # Save room for special tokens\n",
    "    for token, freq in ordered_dict.items():\n",
    "        if freq >= min_freq:\n",
    "            tokens.append((token, freq))\n",
    "\n",
    "    specials = (specials, np.nan)\n",
    "    tokens[0] = specials\n",
    "\n",
    "    return Vocab(tokens, specials)\n",
    "\n",
    "def pipeline(word, vocab, tokenizer):\n",
    "    return vocab(tokenizer(word))\n",
    "    \n",
    "def build_vocab(\n",
    "        iterator,\n",
    "        tokenizer, \n",
    "        params: Word2VecParams,\n",
    "        max_tokens: Optional[int] = None,\n",
    "    ):\n",
    "    counter = Counter()\n",
    "    for tokens in yield_tokens(iterator, tokenizer):\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # First sort by descending frequency, then lexicographically\n",
    "    sorted_by_freq_tuples = sorted(\n",
    "        counter.items(), key=lambda x: (-x[1], x[0])\n",
    "        )\n",
    "\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "    word_vocab = _build_vocab(\n",
    "        ordered_dict, min_freq=params.MIN_FREQ, specials=params.SPECIALS\n",
    "        )\n",
    "    return word_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGrams:\n",
    "    def __init__(self, vocab: Vocab, params: Word2VecParams, tokenizer):\n",
    "        self.vocab = vocab\n",
    "        self.params = params\n",
    "        self.t = self._t()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.discard_probs = self._create_discard_dict()\n",
    "\n",
    "    def _t(self):\n",
    "        freq_list = []\n",
    "        for _, (_, freq) in list(self.vocab.stoi.items())[1:]:\n",
    "            freq_list.append(freq/self.vocab.total_tokens)\n",
    "        return np.percentile(freq_list, self.params.T)\n",
    "        \n",
    "\n",
    "    def _create_discard_dict(self):\n",
    "        discard_dict = {}\n",
    "        for _, (word, freq) in self.vocab.stoi.items():\n",
    "            dicard_prob = 1-np.sqrt(\n",
    "                self.t / (freq/self.vocab.total_tokens + self.t))\n",
    "            discard_dict[word] = dicard_prob\n",
    "        return discard_dict\n",
    "        \n",
    "\n",
    "    def collate_skipgram(self, batch):\n",
    "        batch_input, batch_output  = [], []\n",
    "        for text in batch:\n",
    "            text_tokens = self.vocab.get_index(self.tokenizer(text))\n",
    "\n",
    "            if len(text_tokens) < self.params.SKIPGRAM_N_WORDS * 2 + 1:\n",
    "                continue\n",
    "\n",
    "            for idx in range(len(text_tokens) - self.params.SKIPGRAM_N_WORDS*2\n",
    "                ):\n",
    "                token_id_sequence = text_tokens[\n",
    "                    idx : (idx + self.params.SKIPGRAM_N_WORDS * 2 + 1)\n",
    "                    ]\n",
    "                input_ = token_id_sequence.pop(self.params.SKIPGRAM_N_WORDS)\n",
    "                outputs = token_id_sequence\n",
    "\n",
    "                prb = random.random()\n",
    "                del_pair = self.discard_probs.get(input_)\n",
    "                if input_==0 or del_pair >= prb:\n",
    "                    continue\n",
    "                else:\n",
    "                    for output in outputs:\n",
    "                        prb = random.random()\n",
    "                        del_pair = self.discard_probs.get(output)\n",
    "                        if output==0 or del_pair >= prb:\n",
    "                            continue\n",
    "                        else:\n",
    "                            batch_input.append(input_)\n",
    "                            batch_output.append(output)\n",
    "\n",
    "        batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "        batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "        \n",
    "        return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    def __init__(self, vocab: Vocab, ns_exponent: float, ns_array_len: int):\n",
    "        self.vocab = vocab\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.ns_array_len = ns_array_len\n",
    "        self.ns_array = self._create_negative_sampling()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ns_array)\n",
    "\n",
    "    def _create_negative_sampling(self):\n",
    "\n",
    "        frequency_dict = {word:freq**(self.ns_exponent) \\\n",
    "                          for _,(word, freq) in \n",
    "                          list(self.vocab.stoi.items())[1:]}\n",
    "        frequency_dict_scaled = {\n",
    "            word: \n",
    "            max(1,int((freq/self.vocab.total_tokens)*self.ns_array_len)) \n",
    "            for word, freq in frequency_dict.items()\n",
    "            }\n",
    "        ns_array = []\n",
    "        for word, freq in tqdm(frequency_dict_scaled.items()):\n",
    "            ns_array = ns_array + [word]*freq\n",
    "        return ns_array\n",
    "\n",
    "    def sample(self,n_batches: int=1, n_samples: int=1):\n",
    "        samples = []\n",
    "        for _ in range(n_batches):\n",
    "            samples.append(random.sample(self.ns_array, n_samples))\n",
    "        samples = torch.as_tensor(np.array(samples))\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab: Vocab, params: Word2VecParams):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.t_embeddings = nn.Embedding(\n",
    "            self.vocab.__len__()+1, \n",
    "            params.EMBED_DIM, \n",
    "            max_norm=params.EMBED_MAX_NORM\n",
    "            )\n",
    "        self.c_embeddings = nn.Embedding(\n",
    "            self.vocab.__len__()+1, \n",
    "            params.EMBED_DIM, \n",
    "            max_norm=params.EMBED_MAX_NORM\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs, context):\n",
    "        # getting embeddings for target & reshaping \n",
    "        target_embeddings = self.t_embeddings(inputs)\n",
    "        n_examples = target_embeddings.shape[0]\n",
    "        n_dimensions = target_embeddings.shape[1]\n",
    "        target_embeddings = target_embeddings.view(n_examples, 1, n_dimensions)\n",
    "\n",
    "        # get embeddings for context labels & reshaping \n",
    "        # Allows us to do a bunch of matrix multiplications\n",
    "        context_embeddings = self.c_embeddings(context)\n",
    "        # * This transposes each batch\n",
    "        context_embeddings = context_embeddings.permute(0,2,1)\n",
    "\n",
    "        # * custom linear layer\n",
    "        dots = target_embeddings.bmm(context_embeddings)\n",
    "        dots = dots.view(dots.shape[0], dots.shape[2])\n",
    "        return dots \n",
    "\n",
    "    def normalize_embeddings(self):\n",
    "        embeddings = list(self.t_embeddings.parameters())[0]\n",
    "        embeddings = embeddings.cpu().detach().numpy() \n",
    "        norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "        norms = norms.reshape(norms.shape[0], 1)\n",
    "        return embeddings / norms\n",
    "\n",
    "    def get_similar_words(self, word, n):\n",
    "        word_id = self.vocab.get_index(word)\n",
    "        if word_id == 0:\n",
    "            print(\"Out of vocabulary word\")\n",
    "            return\n",
    "\n",
    "        embedding_norms = self.normalize_embeddings()\n",
    "        word_vec = embedding_norms[word_id]\n",
    "        word_vec = np.reshape(word_vec, (word_vec.shape[0], 1))\n",
    "        dists = np.matmul(embedding_norms, word_vec).flatten()\n",
    "        topN_ids = np.argsort(-dists)[1 : n + 1]\n",
    "\n",
    "        topN_dict = {}\n",
    "        for sim_word_id in topN_ids:\n",
    "            sim_word = self.vocab.lookup_token(sim_word_id)\n",
    "            topN_dict[sim_word] = dists[sim_word_id]\n",
    "        return topN_dict\n",
    "\n",
    "    def get_similarity(self, word1, word2):\n",
    "        idx1 = self.vocab.get_index(word1)\n",
    "        idx2 = self.vocab.get_index(word2)\n",
    "        if idx1 == 0 or idx2 == 0:\n",
    "            print(\"One or both words are out of vocabulary\")\n",
    "            return\n",
    "        \n",
    "        embedding_norms = self.normalize_embeddings()\n",
    "        word1_vec, word2_vec = embedding_norms[idx1], embedding_norms[idx2]\n",
    " \n",
    "        return cosine(word1_vec, word2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: Model, params: Word2VecParams, optimizer,\n",
    "                vocab: Vocab, train_iter, valid_iter, skipgrams: SkipGrams):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.vocab = vocab\n",
    "        self.train_iter = train_iter\n",
    "        self.valid_iter = valid_iter\n",
    "        self.skipgrams = skipgrams\n",
    "        self.params = params\n",
    "\n",
    "        self.epoch_train_mins = {}\n",
    "        self.loss = {\"train\": [], \"valid\": []}\n",
    "\n",
    "        # sending all to device\n",
    "        self.model.to(self.params.DEVICE)\n",
    "        self.params.CRITERION.to(self.params.DEVICE)\n",
    "\n",
    "        self.negative_sampler = NegativeSampler(\n",
    "            vocab=self.vocab, ns_exponent=.75, \n",
    "            ns_array_len=self.params.NS_ARRAY_LEN\n",
    "            )\n",
    "        self.testwords = ['love', 'hurricane', 'military', 'army']\n",
    "    \n",
    "\n",
    "    def train(self):\n",
    "        self.test_testwords()\n",
    "        for epoch in range(self.params.N_EPOCHS):\n",
    "            # Generate Dataloaders\n",
    "            self.train_dataloader = DataLoader(\n",
    "                self.train_iter,\n",
    "                batch_size=self.params.BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                collate_fn=self.skipgrams.collate_skipgram\n",
    "            )\n",
    "            self.valid_dataloader = DataLoader(\n",
    "                self.valid_iter,\n",
    "                batch_size=self.params.BATCH_SIZE,\n",
    "                shuffle=False,\n",
    "                collate_fn=self.skipgrams.collate_skipgram\n",
    "            )\n",
    "            # training the model\n",
    "            st_time = monotonic()\n",
    "            self._train_epoch()\n",
    "            self.epoch_train_mins[epoch] = round((monotonic()-st_time)/60, 1)\n",
    "\n",
    "            # validating the model\n",
    "            self._validate_epoch()\n",
    "            print(f\"\"\"\\nEpoch: {epoch+1}/{self.params.N_EPOCHS}\\n\"\"\", \n",
    "            f\"\"\"    Train Loss: {self.loss['train'][-1]:.2}\\n\"\"\",\n",
    "            f\"\"\"    Valid Loss: {self.loss['valid'][-1]:.2}\\n\"\"\",\n",
    "            f\"\"\"    Training Time (mins): {self.epoch_train_mins.get(epoch)}\"\"\"\n",
    "            )\n",
    "            self.test_testwords()\n",
    "\n",
    "                \n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch_data in tqdm(enumerate(self.train_dataloader, 1), total=len(self.train_dataloader)):\n",
    "            if len(batch_data[0]) == 0:\n",
    "                continue\n",
    "            inputs = batch_data[0].to(self.params.DEVICE)\n",
    "            pos_labels = batch_data[1].to(self.params.DEVICE)\n",
    "            neg_labels = self.negative_sampler.sample(\n",
    "                pos_labels.shape[0], self.params.NEG_SAMPLES\n",
    "                )\n",
    "            neg_labels = neg_labels.to(self.params.DEVICE)\n",
    "            context = torch.cat(\n",
    "                [pos_labels.view(pos_labels.shape[0], 1), \n",
    "                neg_labels], dim=1\n",
    "              )            \n",
    "\n",
    "            # building the targets tensor  \n",
    "            y_pos = torch.ones((pos_labels.shape[0], 1))\n",
    "            y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
    "            y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(inputs, context)\n",
    "            loss = self.params.CRITERION(outputs, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        \n",
    "        self.loss['train'].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(self.valid_dataloader, 1):\n",
    "                if len(batch_data[0]) == 0:\n",
    "                    continue\n",
    "                inputs = batch_data[0].to(self.params.DEVICE)\n",
    "                pos_labels = batch_data[1].to(self.params.DEVICE)\n",
    "                neg_labels = self.negative_sampler.sample(\n",
    "                    pos_labels.shape[0], self.params.NEG_SAMPLES\n",
    "                    ).to(self.params.DEVICE)\n",
    "                context = torch.cat(\n",
    "                    [pos_labels.view(pos_labels.shape[0], 1), \n",
    "                    neg_labels], dim=1\n",
    "                  )\n",
    "\n",
    "\n",
    "                # building the targets tensor  \n",
    "                y_pos = torch.ones((pos_labels.shape[0], 1))\n",
    "                y_neg = torch.zeros((neg_labels.shape[0], neg_labels.shape[1]))\n",
    "                y = torch.cat([y_pos, y_neg], dim=1).to(self.params.DEVICE)\n",
    "\n",
    "                preds = self.model(inputs, context).to(self.params.DEVICE)\n",
    "                loss = self.params.CRITERION(preds, y)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "            epoch_loss = np.mean(running_loss)\n",
    "            self.loss['valid'].append(epoch_loss)\n",
    "\n",
    "    def test_testwords(self, n: int = 5):\n",
    "        for word in self.testwords:\n",
    "            print(word, end=\": \")\n",
    "            nn_words = self.model.get_similar_words(word, n)\n",
    "            for w, sim in nn_words.items():\n",
    "                print(f\"{w} ({sim:.3})\", end=' ')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': (0, nan),\n",
       " 'of': (1, 57030),\n",
       " 'and': (2, 50735),\n",
       " 'in': (3, 45015),\n",
       " 'to': (4, 39521),\n",
       " 'a': (5, 36523),\n",
       " 'was': (6, 21008),\n",
       " 'on': (7, 15140),\n",
       " 'as': (8, 15058),\n",
       " 's': (9, 14936),\n",
       " 'that': (10, 14351),\n",
       " 'for': (11, 13794),\n",
       " 'with': (12, 13012),\n",
       " 'by': (13, 12718),\n",
       " 'is': (14, 11691),\n",
       " 'it': (15, 9273),\n",
       " 'from': (16, 9229),\n",
       " 'at': (17, 9070),\n",
       " 'his': (18, 9019),\n",
       " 'he': (19, 8706),\n",
       " 'were': (20, 7334),\n",
       " 'an': (21, 6250),\n",
       " 'had': (22, 5707),\n",
       " 'which': (23, 5546),\n",
       " 'be': (24, 4859),\n",
       " 'are': (25, 4714),\n",
       " 'this': (26, 4560),\n",
       " 'their': (27, 4290),\n",
       " 'first': (28, 4242),\n",
       " 'but': (29, 4233),\n",
       " 'not': (30, 4006),\n",
       " 'one': (31, 3910),\n",
       " 'they': (32, 3894),\n",
       " 'its': (33, 3877),\n",
       " 'also': (34, 3842),\n",
       " 'after': (35, 3749),\n",
       " 'her': (36, 3670),\n",
       " 'or': (37, 3655),\n",
       " 'two': (38, 3565),\n",
       " 'have': (39, 3470),\n",
       " 'has': (40, 3325),\n",
       " 'been': (41, 3263),\n",
       " 'who': (42, 3029),\n",
       " 'she': (43, 2884),\n",
       " 'new': (44, 2767),\n",
       " 'other': (45, 2729),\n",
       " 'during': (46, 2690),\n",
       " 'when': (47, 2655),\n",
       " 'time': (48, 2607),\n",
       " 'all': (49, 2557),\n",
       " 'into': (50, 2443),\n",
       " 'more': (51, 2402),\n",
       " 'would': (52, 2332),\n",
       " '1': (53, 2284),\n",
       " 'i': (54, 2248),\n",
       " 'over': (55, 2137),\n",
       " 'while': (56, 2127),\n",
       " 'game': (57, 2077),\n",
       " 'only': (58, 2061),\n",
       " 'most': (59, 2027),\n",
       " '2': (60, 1988),\n",
       " 'three': (61, 1976),\n",
       " 'later': (62, 1928),\n",
       " 'about': (63, 1922),\n",
       " 'up': (64, 1918),\n",
       " 'may': (65, 1877),\n",
       " 'between': (66, 1871),\n",
       " 'him': (67, 1848),\n",
       " 'song': (68, 1828),\n",
       " 'there': (69, 1801),\n",
       " 'some': (70, 1771),\n",
       " 'than': (71, 1765),\n",
       " 'out': (72, 1760),\n",
       " 'no': (73, 1702),\n",
       " 'season': (74, 1692),\n",
       " 'year': (75, 1668),\n",
       " 'made': (76, 1642),\n",
       " 'city': (77, 1609),\n",
       " '3': (78, 1601),\n",
       " 'such': (79, 1584),\n",
       " 'before': (80, 1557),\n",
       " 'where': (81, 1524),\n",
       " 'used': (82, 1508),\n",
       " 'series': (83, 1498),\n",
       " 'them': (84, 1488),\n",
       " 'second': (85, 1465),\n",
       " 'world': (86, 1462),\n",
       " 'being': (87, 1454),\n",
       " 'years': (88, 1451),\n",
       " 'both': (89, 1448),\n",
       " 'many': (90, 1447),\n",
       " 'these': (91, 1445),\n",
       " 'film': (92, 1405),\n",
       " 'however': (93, 1397),\n",
       " 'album': (94, 1383),\n",
       " 'south': (95, 1373),\n",
       " 'war': (96, 1370),\n",
       " 'through': (97, 1364),\n",
       " '5': (98, 1361),\n",
       " 'north': (99, 1357),\n",
       " 'then': (100, 1329),\n",
       " 'can': (101, 1320),\n",
       " 'part': (102, 1319),\n",
       " 'early': (103, 1299),\n",
       " 'several': (104, 1287),\n",
       " '4': (105, 1279),\n",
       " 'number': (106, 1278),\n",
       " 'state': (107, 1274),\n",
       " 'including': (108, 1273),\n",
       " 'against': (109, 1272),\n",
       " 'well': (110, 1262),\n",
       " 'known': (111, 1259),\n",
       " 'became': (112, 1255),\n",
       " 'm': (113, 1239),\n",
       " 'four': (114, 1236),\n",
       " 'united': (115, 1216),\n",
       " 'under': (116, 1198),\n",
       " 'although': (117, 1194),\n",
       " 'century': (118, 1178),\n",
       " 'day': (119, 1165),\n",
       " 'following': (120, 1152),\n",
       " 'music': (121, 1134),\n",
       " 'began': (122, 1102),\n",
       " 'because': (123, 1090),\n",
       " 'so': (124, 1083),\n",
       " 'work': (125, 1083),\n",
       " 'like': (126, 1075),\n",
       " 'end': (127, 1071),\n",
       " 'called': (128, 1065),\n",
       " 'episode': (129, 1064),\n",
       " 'until': (130, 1040),\n",
       " 'found': (131, 1039),\n",
       " 'said': (132, 1038),\n",
       " 'area': (133, 1034),\n",
       " 'could': (134, 1033),\n",
       " 'states': (135, 1019),\n",
       " 'american': (136, 1018),\n",
       " 'people': (137, 1015),\n",
       " '6': (138, 1014),\n",
       " 'since': (139, 995),\n",
       " 'british': (140, 989),\n",
       " 'each': (141, 984),\n",
       " 'released': (142, 979),\n",
       " 'same': (143, 979),\n",
       " 'team': (144, 974),\n",
       " 'church': (145, 968),\n",
       " '10': (146, 966),\n",
       " 'around': (147, 966),\n",
       " 'long': (148, 960),\n",
       " 'did': (149, 958),\n",
       " 'along': (150, 957),\n",
       " 'million': (151, 940),\n",
       " 'five': (152, 938),\n",
       " 'life': (153, 938),\n",
       " 'national': (154, 916),\n",
       " 'back': (155, 910),\n",
       " 'john': (156, 909),\n",
       " 'high': (157, 903),\n",
       " 'company': (158, 900),\n",
       " 't': (159, 900),\n",
       " 'another': (160, 894),\n",
       " 'best': (161, 891),\n",
       " 'use': (162, 888),\n",
       " 'you': (163, 877),\n",
       " 'if': (164, 872),\n",
       " 'final': (165, 871),\n",
       " 'september': (166, 869),\n",
       " 'august': (167, 865),\n",
       " 'river': (168, 861),\n",
       " 'large': (169, 856),\n",
       " 'what': (170, 846),\n",
       " 'west': (171, 843),\n",
       " '8': (172, 837),\n",
       " 'km': (173, 836),\n",
       " 'off': (174, 835),\n",
       " 'down': (175, 831),\n",
       " '7': (176, 825),\n",
       " 'due': (177, 819),\n",
       " 'games': (178, 817),\n",
       " 'june': (179, 810),\n",
       " 'line': (180, 809),\n",
       " 'history': (181, 808),\n",
       " 'will': (182, 803),\n",
       " 'name': (183, 799),\n",
       " 'now': (184, 791),\n",
       " 'any': (185, 786),\n",
       " 'storm': (186, 786),\n",
       " 'home': (187, 784),\n",
       " 'received': (188, 783),\n",
       " '9': (189, 777),\n",
       " 'described': (190, 774),\n",
       " 'government': (191, 774),\n",
       " 'six': (192, 771),\n",
       " 'species': (193, 770),\n",
       " 'within': (194, 770),\n",
       " 'much': (195, 769),\n",
       " 'group': (196, 764),\n",
       " 'family': (197, 758),\n",
       " 'october': (198, 757),\n",
       " 'played': (199, 755),\n",
       " 'east': (200, 748),\n",
       " 'league': (201, 747),\n",
       " 'general': (202, 744),\n",
       " 'set': (203, 744),\n",
       " 'took': (204, 744),\n",
       " 'major': (205, 737),\n",
       " 'road': (206, 737),\n",
       " 'july': (207, 733),\n",
       " 'wrote': (208, 732),\n",
       " 'late': (209, 731),\n",
       " 'single': (210, 731),\n",
       " 'won': (211, 731),\n",
       " 'system': (212, 729),\n",
       " 'play': (213, 725),\n",
       " 'video': (214, 725),\n",
       " 'times': (215, 724),\n",
       " 'us': (216, 724),\n",
       " 'according': (217, 723),\n",
       " 'record': (218, 718),\n",
       " 'third': (219, 716),\n",
       " 'based': (220, 713),\n",
       " 'april': (221, 711),\n",
       " 'man': (222, 707),\n",
       " 'included': (223, 702),\n",
       " 'just': (224, 700),\n",
       " 'march': (225, 699),\n",
       " 'book': (226, 697),\n",
       " 'those': (227, 694),\n",
       " 'january': (228, 693),\n",
       " 'show': (229, 693),\n",
       " 'named': (230, 691),\n",
       " 'even': (231, 690),\n",
       " 'very': (232, 690),\n",
       " 'england': (233, 689),\n",
       " 'main': (234, 687),\n",
       " 'white': (235, 687),\n",
       " 'left': (236, 686),\n",
       " 'york': (237, 685),\n",
       " 'men': (238, 684),\n",
       " 'school': (239, 681),\n",
       " 'small': (240, 681),\n",
       " 'though': (241, 678),\n",
       " 'division': (242, 672),\n",
       " 'club': (243, 671),\n",
       " 'way': (244, 669),\n",
       " 'old': (245, 668),\n",
       " 'original': (246, 667),\n",
       " 'near': (247, 666),\n",
       " 'last': (248, 665),\n",
       " '12': (249, 662),\n",
       " 'november': (250, 661),\n",
       " 'water': (251, 657),\n",
       " 'death': (252, 654),\n",
       " 'place': (253, 654),\n",
       " '20': (254, 653),\n",
       " '15': (255, 650),\n",
       " 'tropical': (256, 650),\n",
       " 'december': (257, 647),\n",
       " 'built': (258, 646),\n",
       " 'own': (259, 644),\n",
       " 'character': (260, 641),\n",
       " 'we': (261, 641),\n",
       " 'songs': (262, 639),\n",
       " 'top': (263, 633),\n",
       " 'de': (264, 632),\n",
       " 'form': (265, 631),\n",
       " '30': (266, 630),\n",
       " 'player': (267, 630),\n",
       " 'do': (268, 625),\n",
       " 'king': (269, 625),\n",
       " 'black': (270, 624),\n",
       " 'public': (271, 623),\n",
       " 'german': (272, 619),\n",
       " 'island': (273, 618),\n",
       " 'next': (274, 617),\n",
       " '2009': (275, 616),\n",
       " 'make': (276, 613),\n",
       " '2008': (277, 612),\n",
       " 'still': (278, 612),\n",
       " '2010': (279, 611),\n",
       " 'u': (280, 600),\n",
       " 'role': (281, 598),\n",
       " 'led': (282, 596),\n",
       " 'again': (283, 595),\n",
       " 'moved': (284, 590),\n",
       " 'career': (285, 589),\n",
       " 'ii': (286, 589),\n",
       " 'university': (287, 589),\n",
       " 'without': (288, 587),\n",
       " 'love': (289, 585),\n",
       " 'often': (290, 584),\n",
       " 'among': (291, 582),\n",
       " 'recorded': (292, 581),\n",
       " 'further': (293, 578),\n",
       " 'hurricane': (294, 578),\n",
       " 'military': (295, 576),\n",
       " 'period': (296, 575),\n",
       " 'star': (297, 575),\n",
       " 'local': (298, 574),\n",
       " 'considered': (299, 573),\n",
       " 'army': (300, 570),\n",
       " 'production': (301, 570),\n",
       " 'release': (302, 569),\n",
       " 'side': (303, 568),\n",
       " '2007': (304, 567),\n",
       " 'great': (305, 565),\n",
       " 'house': (306, 557),\n",
       " 'came': (307, 554),\n",
       " 'published': (308, 554),\n",
       " 'written': (309, 552),\n",
       " '100': (310, 551),\n",
       " 'continued': (311, 550),\n",
       " 'power': (312, 549),\n",
       " 'town': (313, 547),\n",
       " 'english': (314, 546),\n",
       " 'story': (315, 545),\n",
       " 'days': (316, 542),\n",
       " 'forces': (317, 542),\n",
       " 'run': (318, 541),\n",
       " 'held': (319, 540),\n",
       " 'route': (320, 540),\n",
       " 'french': (321, 539),\n",
       " 'support': (322, 535),\n",
       " '14': (323, 534),\n",
       " '16': (324, 531),\n",
       " '11': (325, 529),\n",
       " 'force': (326, 527),\n",
       " 'half': (327, 527),\n",
       " 'few': (328, 526),\n",
       " 'take': (329, 526),\n",
       " 'international': (330, 525),\n",
       " 'having': (331, 524),\n",
       " '25': (332, 523),\n",
       " 'county': (333, 523),\n",
       " 'land': (334, 521),\n",
       " 'throughout': (335, 521),\n",
       " '2011': (336, 519),\n",
       " 'point': (337, 518),\n",
       " '18': (338, 516),\n",
       " 'become': (339, 516),\n",
       " '2006': (340, 515),\n",
       " 'children': (341, 515),\n",
       " 'order': (342, 515),\n",
       " 'light': (343, 513),\n",
       " 'version': (344, 513),\n",
       " 'title': (345, 511),\n",
       " 'former': (346, 509),\n",
       " 'lost': (347, 507),\n",
       " 'track': (348, 507),\n",
       " 'different': (349, 506),\n",
       " 'development': (350, 505),\n",
       " 'field': (351, 504),\n",
       " 'ship': (352, 503),\n",
       " 'similar': (353, 502),\n",
       " 'despite': (354, 499),\n",
       " 'live': (355, 498),\n",
       " 'common': (356, 497),\n",
       " 'members': (357, 496),\n",
       " 'park': (358, 494),\n",
       " 'c': (359, 492),\n",
       " 'february': (360, 492),\n",
       " '13': (361, 491),\n",
       " 'gave': (362, 488),\n",
       " 'produced': (363, 488),\n",
       " 'short': (364, 487),\n",
       " 'southern': (365, 487),\n",
       " 'dylan': (366, 485),\n",
       " 'little': (367, 485),\n",
       " 'site': (368, 482),\n",
       " '2012': (369, 480),\n",
       " 'once': (370, 480),\n",
       " 'television': (371, 480),\n",
       " 'writing': (372, 480),\n",
       " 'given': (373, 479),\n",
       " 'central': (374, 478),\n",
       " 'control': (375, 476),\n",
       " 'total': (376, 476),\n",
       " 'band': (377, 475),\n",
       " 'country': (378, 475),\n",
       " 'service': (379, 472),\n",
       " 'northern': (380, 470),\n",
       " 're': (381, 469),\n",
       " 'include': (382, 465),\n",
       " 'young': (383, 464),\n",
       " 'fire': (384, 463),\n",
       " 'position': (385, 463),\n",
       " 'battalion': (386, 460),\n",
       " 'making': (387, 457),\n",
       " 'never': (388, 457),\n",
       " 'away': (389, 456),\n",
       " 'seven': (390, 455),\n",
       " 'tour': (391, 455),\n",
       " 'age': (392, 454),\n",
       " 'air': (393, 454),\n",
       " 'lead': (394, 454),\n",
       " '2013': (395, 452),\n",
       " 'how': (396, 451),\n",
       " 'open': (397, 450),\n",
       " 'reported': (398, 450),\n",
       " 'seen': (399, 450),\n",
       " 'battle': (400, 449),\n",
       " 'highway': (401, 449),\n",
       " 'eastern': (402, 448),\n",
       " 'good': (403, 448),\n",
       " 'western': (404, 448),\n",
       " 'stated': (405, 446),\n",
       " 'attack': (406, 445),\n",
       " 'red': (407, 445),\n",
       " 'god': (408, 444),\n",
       " 'h': (409, 444),\n",
       " 'match': (410, 444),\n",
       " 'across': (411, 443),\n",
       " 'st': (412, 443),\n",
       " 'body': (413, 442),\n",
       " 'instead': (414, 442),\n",
       " 'returned': (415, 441),\n",
       " 'ships': (416, 441),\n",
       " 'established': (417, 440),\n",
       " 'using': (418, 438),\n",
       " 'ft': (419, 435),\n",
       " 'population': (420, 435),\n",
       " 'america': (421, 434),\n",
       " 'construction': (422, 434),\n",
       " 'modern': (423, 434),\n",
       " 'week': (424, 434),\n",
       " 'noted': (425, 432),\n",
       " 'less': (426, 431),\n",
       " 'my': (427, 431),\n",
       " 'royal': (428, 431),\n",
       " 'head': (429, 430),\n",
       " 'reached': (430, 430),\n",
       " 'building': (431, 429),\n",
       " 'developed': (432, 429),\n",
       " 'eight': (433, 429),\n",
       " 'rock': (434, 428),\n",
       " 'ireland': (435, 427),\n",
       " 'players': (436, 427),\n",
       " 'brigade': (437, 426),\n",
       " 'b': (438, 425),\n",
       " 'president': (439, 424),\n",
       " 'result': (440, 423),\n",
       " 'thought': (441, 422),\n",
       " 'performance': (442, 420),\n",
       " 'right': (443, 420),\n",
       " 'london': (444, 418),\n",
       " 'miles': (445, 418),\n",
       " 'himself': (446, 417),\n",
       " 'father': (447, 416),\n",
       " 'per': (448, 415),\n",
       " 'important': (449, 414),\n",
       " 'style': (450, 413),\n",
       " 'performed': (451, 412),\n",
       " 'felt': (452, 411),\n",
       " 'australia': (453, 410),\n",
       " 'various': (454, 410),\n",
       " '17': (455, 409),\n",
       " 'full': (456, 409),\n",
       " 'areas': (457, 408),\n",
       " 'feet': (458, 408),\n",
       " 'previous': (459, 407),\n",
       " 'events': (460, 406),\n",
       " 'win': (461, 405),\n",
       " 'low': (462, 404),\n",
       " 'died': (463, 401),\n",
       " 'kingdom': (464, 401),\n",
       " 'guitar': (465, 400),\n",
       " 'football': (466, 399),\n",
       " 'others': (467, 398),\n",
       " 'art': (468, 397),\n",
       " 'mm': (469, 397),\n",
       " 'originally': (470, 397),\n",
       " 'project': (471, 397),\n",
       " 'too': (472, 397),\n",
       " 'went': (473, 397),\n",
       " 'human': (474, 396),\n",
       " '23': (475, 395),\n",
       " 'level': (476, 395),\n",
       " 'upon': (477, 395),\n",
       " 'range': (478, 394),\n",
       " 'works': (479, 394),\n",
       " 'formed': (480, 391),\n",
       " 'started': (481, 391),\n",
       " 'characters': (482, 390),\n",
       " 'james': (483, 390),\n",
       " 'political': (484, 390),\n",
       " 'women': (485, 388),\n",
       " 'should': (486, 387),\n",
       " 'cup': (487, 386),\n",
       " 'port': (488, 384),\n",
       " '50': (489, 383),\n",
       " 'caused': (490, 383),\n",
       " '21': (491, 382),\n",
       " '28': (492, 382),\n",
       " 'eventually': (493, 382),\n",
       " 'located': (494, 382),\n",
       " '19': (495, 380),\n",
       " '24': (496, 378),\n",
       " 'stars': (497, 378),\n",
       " 'critics': (498, 377),\n",
       " 'ground': (499, 377),\n",
       " 'sent': (500, 377),\n",
       " 'able': (501, 376),\n",
       " 'created': (502, 376),\n",
       " '2004': (503, 374),\n",
       " 'me': (504, 374),\n",
       " '2005': (505, 373),\n",
       " 'class': (506, 373),\n",
       " 'd': (507, 372),\n",
       " 'chart': (508, 371),\n",
       " 'night': (509, 371),\n",
       " 'born': (510, 369),\n",
       " 'region': (511, 369),\n",
       " 'street': (512, 367),\n",
       " 'center': (513, 366),\n",
       " 'court': (514, 366),\n",
       " 'design': (515, 366),\n",
       " 'together': (516, 366),\n",
       " 'director': (517, 365),\n",
       " 'popular': (518, 364),\n",
       " 'present': (519, 364),\n",
       " 'strong': (520, 364),\n",
       " 'award': (521, 363),\n",
       " 'every': (522, 363),\n",
       " 'return': (523, 361),\n",
       " 'son': (524, 361),\n",
       " 'hero': (525, 360),\n",
       " 'remained': (526, 360),\n",
       " 'see': (527, 359),\n",
       " 'completed': (528, 358),\n",
       " 'guns': (529, 358),\n",
       " 'novel': (530, 358),\n",
       " 'scored': (531, 357),\n",
       " 'announced': (532, 355),\n",
       " 'australian': (533, 355),\n",
       " 'grand': (534, 354),\n",
       " '22': (535, 353),\n",
       " 'almost': (536, 353),\n",
       " 'fourth': (537, 353),\n",
       " 'behind': (538, 350),\n",
       " 'damage': (539, 350),\n",
       " 'least': (540, 350),\n",
       " '26': (541, 349),\n",
       " 'brown': (542, 348),\n",
       " 'party': (543, 348),\n",
       " 'ten': (544, 348),\n",
       " 'added': (545, 347),\n",
       " 'heavy': (546, 347),\n",
       " 'followed': (547, 345),\n",
       " 'months': (548, 345),\n",
       " 'appeared': (549, 344),\n",
       " 'wife': (550, 344),\n",
       " 'killed': (551, 343),\n",
       " 'addition': (552, 342),\n",
       " 'does': (553, 342),\n",
       " 'playing': (554, 342),\n",
       " 'success': (555, 342),\n",
       " 'awards': (556, 340),\n",
       " 'list': (557, 340),\n",
       " 'features': (558, 338),\n",
       " 'aircraft': (559, 335),\n",
       " '2003': (560, 334),\n",
       " 'coast': (561, 334),\n",
       " 'sea': (562, 334),\n",
       " 'taken': (563, 333),\n",
       " '2015': (564, 331),\n",
       " 'david': (565, 331),\n",
       " 'leading': (566, 330),\n",
       " 'championship': (567, 329),\n",
       " 'action': (568, 328),\n",
       " 'france': (569, 328),\n",
       " 'either': (570, 327),\n",
       " 'europe': (571, 327),\n",
       " 'front': (572, 327),\n",
       " 'recording': (573, 327),\n",
       " 'served': (574, 327),\n",
       " 'towards': (575, 326),\n",
       " 'campaign': (576, 325),\n",
       " 'operations': (577, 325),\n",
       " 'gold': (578, 324),\n",
       " 'mother': (579, 323),\n",
       " 'put': (580, 323),\n",
       " 'decided': (581, 322),\n",
       " 'elements': (582, 322),\n",
       " 'close': (583, 320),\n",
       " 'records': (584, 320),\n",
       " 'believed': (585, 319),\n",
       " 'fleet': (586, 319),\n",
       " 'generally': (587, 319),\n",
       " 'magazine': (588, 319),\n",
       " 'carey': (589, 316),\n",
       " 'ever': (590, 316),\n",
       " 'female': (591, 316),\n",
       " 'post': (592, 316),\n",
       " 'poem': (593, 315),\n",
       " 'sold': (594, 315),\n",
       " 'soon': (595, 315),\n",
       " 'example': (596, 313),\n",
       " 'infantry': (597, 313),\n",
       " 'points': (598, 313),\n",
       " 'significant': (599, 313),\n",
       " 'fort': (600, 312),\n",
       " 'goal': (601, 312),\n",
       " 'move': (602, 312),\n",
       " 'weeks': (603, 312),\n",
       " 'rather': (604, 311),\n",
       " 'study': (605, 311),\n",
       " 'european': (606, 310),\n",
       " 'federer': (607, 309),\n",
       " 'outside': (608, 309),\n",
       " 'o': (609, 308),\n",
       " 'opened': (610, 308),\n",
       " 'robert': (611, 308),\n",
       " 'case': (612, 307),\n",
       " 'directed': (613, 307),\n",
       " 'brought': (614, 306),\n",
       " 'help': (615, 306),\n",
       " 'law': (616, 306),\n",
       " 'non': (617, 306),\n",
       " 'finished': (618, 305),\n",
       " '27': (619, 304),\n",
       " 'earlier': (620, 304),\n",
       " 'wales': (621, 304),\n",
       " 'william': (622, 304),\n",
       " 'featured': (623, 303),\n",
       " 'go': (624, 303),\n",
       " 'get': (625, 302),\n",
       " 'victory': (626, 302),\n",
       " 'manager': (627, 300),\n",
       " 'successful': (628, 300),\n",
       " 'act': (629, 299),\n",
       " 'gun': (630, 299),\n",
       " 'stage': (631, 298),\n",
       " 'association': (632, 297),\n",
       " 'member': (633, 297),\n",
       " 'provided': (634, 297),\n",
       " 'mi': (635, 296),\n",
       " 'mid': (636, 296),\n",
       " 'opening': (637, 296),\n",
       " 'start': (638, 296),\n",
       " 'village': (639, 296),\n",
       " 'working': (640, 296),\n",
       " 'council': (641, 295),\n",
       " 'wanted': (642, 295),\n",
       " 'appearance': (643, 294),\n",
       " 'jordan': (644, 293),\n",
       " 'particularly': (645, 293),\n",
       " 'roman': (646, 293),\n",
       " 'troops': (647, 292),\n",
       " '2014': (648, 291),\n",
       " '40': (649, 291),\n",
       " 'atlantic': (650, 291),\n",
       " 'depression': (651, 291),\n",
       " 'initially': (652, 291),\n",
       " 'tech': (653, 291),\n",
       " '29': (654, 290),\n",
       " 'evidence': (655, 290),\n",
       " 'yard': (656, 290),\n",
       " 'far': (657, 289),\n",
       " 'find': (658, 289),\n",
       " 'largest': (659, 289),\n",
       " 'office': (660, 289),\n",
       " 'blue': (661, 288),\n",
       " 'dam': (662, 288),\n",
       " 'george': (663, 288),\n",
       " 'review': (664, 288),\n",
       " 'attempt': (665, 287),\n",
       " 'possible': (666, 287),\n",
       " 'saw': (667, 287),\n",
       " 'special': (668, 286),\n",
       " 'type': (669, 286),\n",
       " 'month': (670, 285),\n",
       " 'summer': (671, 285),\n",
       " '19th': (672, 284),\n",
       " 'above': (673, 284),\n",
       " 'union': (674, 284),\n",
       " 'yards': (675, 284),\n",
       " 'e': (676, 283),\n",
       " 'florida': (677, 283),\n",
       " 'rest': (678, 283),\n",
       " 'allowed': (679, 281),\n",
       " 'event': (680, 281),\n",
       " 'race': (681, 281),\n",
       " 'winds': (682, 281),\n",
       " 'critical': (683, 280),\n",
       " 'saying': (684, 280),\n",
       " 'creek': (685, 279),\n",
       " 'cross': (686, 279),\n",
       " 'hours': (687, 278),\n",
       " 'whom': (688, 278),\n",
       " 'nine': (689, 277),\n",
       " '2001': (690, 276),\n",
       " 'missouri': (691, 276),\n",
       " 'plan': (692, 276),\n",
       " 'police': (693, 276),\n",
       " 'worked': (694, 276),\n",
       " 'community': (695, 275),\n",
       " 'designed': (696, 275),\n",
       " 'reception': (697, 275),\n",
       " 'society': (698, 275),\n",
       " '500': (699, 274),\n",
       " 'previously': (700, 274),\n",
       " 'free': (701, 273),\n",
       " 'forced': (702, 272),\n",
       " 'middle': (703, 272),\n",
       " 'process': (704, 272),\n",
       " 'era': (705, 271),\n",
       " 'operation': (706, 271),\n",
       " 'radio': (707, 271),\n",
       " 'real': (708, 271),\n",
       " 'remains': (709, 271),\n",
       " '200': (710, 270),\n",
       " 'increased': (711, 269),\n",
       " 'official': (712, 269),\n",
       " 'praised': (713, 269),\n",
       " 'research': (714, 269),\n",
       " 'hall': (715, 268),\n",
       " 'lower': (716, 267),\n",
       " 'parliament': (717, 267),\n",
       " 'station': (718, 267),\n",
       " 'come': (719, 266),\n",
       " 'michael': (720, 266),\n",
       " 'relationship': (721, 266),\n",
       " 'command': (722, 265),\n",
       " 'commander': (723, 265),\n",
       " 'hill': (724, 265),\n",
       " 'regiment': (725, 265),\n",
       " 'studio': (726, 265),\n",
       " 'units': (727, 265),\n",
       " 'base': (728, 264),\n",
       " 'taking': (729, 264),\n",
       " 'parts': (730, 263),\n",
       " 'replaced': (731, 263),\n",
       " 'writer': (732, 263),\n",
       " 'ball': (733, 262),\n",
       " 'industry': (734, 262),\n",
       " 'media': (735, 262),\n",
       " 'navy': (736, 262),\n",
       " 'social': (737, 261),\n",
       " 'food': (738, 260),\n",
       " 'r': (739, 260),\n",
       " 'bay': (740, 259),\n",
       " 'co': (741, 259),\n",
       " 'college': (742, 259),\n",
       " 'highest': (743, 259),\n",
       " 'reviews': (744, 259),\n",
       " 'beginning': (745, 258),\n",
       " 'claimed': (746, 258),\n",
       " 'don': (747, 258),\n",
       " 'canada': (748, 257),\n",
       " 'estimated': (749, 257),\n",
       " 'mph': (750, 257),\n",
       " 'museum': (751, 257),\n",
       " 'section': (752, 257),\n",
       " 'goals': (753, 256),\n",
       " 'stone': (754, 256),\n",
       " 'average': (755, 255),\n",
       " 'commercial': (756, 255),\n",
       " 'japanese': (757, 255),\n",
       " 'joined': (758, 255),\n",
       " 'l': (759, 255),\n",
       " 'religious': (760, 255),\n",
       " 'involved': (761, 254),\n",
       " 'oldham': (762, 254),\n",
       " 'placed': (763, 254),\n",
       " 'stories': (764, 254),\n",
       " 'training': (765, 254),\n",
       " 'introduced': (766, 253),\n",
       " 'met': (767, 253),\n",
       " 'shot': (768, 253),\n",
       " 'signed': (769, 253),\n",
       " 'suggested': (770, 253),\n",
       " 'lines': (771, 252),\n",
       " 'sometimes': (772, 252),\n",
       " '31': (773, 251),\n",
       " 'background': (774, 251),\n",
       " 'business': (775, 251),\n",
       " 'face': (776, 251),\n",
       " 'olivier': (777, 251),\n",
       " 'paul': (778, 251),\n",
       " 'today': (779, 251),\n",
       " 'complete': (780, 250),\n",
       " 'going': (781, 250),\n",
       " 'itself': (782, 250),\n",
       " 'scene': (783, 250),\n",
       " 'henry': (784, 249),\n",
       " 'mexico': (785, 249),\n",
       " 'structure': (786, 249),\n",
       " 'additional': (787, 248),\n",
       " 'available': (788, 248),\n",
       " 'give': (789, 248),\n",
       " 'thus': (790, 248),\n",
       " 'cast': (791, 247),\n",
       " 'date': (792, 247),\n",
       " 'horse': (793, 247),\n",
       " 'language': (794, 247),\n",
       " 'loss': (795, 247),\n",
       " 'india': (796, 246),\n",
       " 'nearly': (797, 246),\n",
       " 'sound': (798, 246),\n",
       " 'term': (799, 246),\n",
       " 'whose': (800, 246),\n",
       " 'fifth': (801, 245),\n",
       " 'past': (802, 245),\n",
       " 'thomas': (803, 245),\n",
       " 'approximately': (804, 244),\n",
       " 'indian': (805, 244),\n",
       " 'irish': (806, 244),\n",
       " 'must': (807, 244),\n",
       " 'program': (808, 244),\n",
       " 'already': (809, 243),\n",
       " 'appointed': (810, 243),\n",
       " 'capital': (811, 243),\n",
       " 'entire': (812, 243),\n",
       " 'friends': (813, 243),\n",
       " 'britain': (814, 242),\n",
       " 'damaged': (815, 242),\n",
       " 'native': (816, 242),\n",
       " 'prior': (817, 242),\n",
       " 'shows': (818, 242),\n",
       " 'told': (819, 242),\n",
       " 'forest': (820, 241),\n",
       " 'issue': (821, 241),\n",
       " 'mark': (822, 241),\n",
       " 'names': (823, 241),\n",
       " 'probably': (824, 241),\n",
       " 'turned': (825, 241),\n",
       " 'male': (826, 240),\n",
       " 'sun': (827, 240),\n",
       " 'winning': (828, 239),\n",
       " 'change': (829, 238),\n",
       " 'g': (830, 238),\n",
       " 'our': (831, 238),\n",
       " 'students': (832, 238),\n",
       " 'earth': (833, 237),\n",
       " 'length': (834, 237),\n",
       " 'passed': (835, 237),\n",
       " 'size': (836, 237),\n",
       " 'child': (837, 236),\n",
       " 'civil': (838, 236),\n",
       " 'especially': (839, 236),\n",
       " 'christian': (840, 235),\n",
       " 'enough': (841, 235),\n",
       " 'notes': (842, 235),\n",
       " 'woman': (843, 235),\n",
       " 'chinese': (844, 234),\n",
       " 'failed': (845, 234),\n",
       " 'forward': (846, 234),\n",
       " 'mixed': (847, 234),\n",
       " 'overall': (848, 234),\n",
       " 'running': (849, 234),\n",
       " 'better': (850, 233),\n",
       " 'captain': (851, 233),\n",
       " 'ny': (852, 233),\n",
       " '2002': (853, 232),\n",
       " 'limited': (854, 232),\n",
       " 'future': (855, 231),\n",
       " 'iii': (856, 231),\n",
       " 'minor': (857, 231),\n",
       " 'network': (858, 231),\n",
       " 'arrived': (859, 230),\n",
       " 'changes': (860, 230),\n",
       " 'includes': (861, 230),\n",
       " 'might': (862, 230),\n",
       " 'moving': (863, 230),\n",
       " 'ordered': (864, 230),\n",
       " 'pacific': (865, 230),\n",
       " 'regular': (866, 230),\n",
       " 'spent': (867, 230),\n",
       " 'wheeler': (868, 230),\n",
       " 'canadian': (869, 229),\n",
       " 'cathedral': (870, 229),\n",
       " 'education': (871, 229),\n",
       " 'larger': (872, 229),\n",
       " 'remaining': (873, 229),\n",
       " 'usually': (874, 229),\n",
       " 'birds': (875, 228),\n",
       " 'department': (876, 228),\n",
       " 'hand': (877, 228),\n",
       " 'hit': (878, 228),\n",
       " 'lake': (879, 227),\n",
       " 'required': (880, 227),\n",
       " 'san': (881, 227),\n",
       " 'uk': (882, 227),\n",
       " 'decision': (883, 226),\n",
       " 'latter': (884, 226),\n",
       " 'africa': (885, 225),\n",
       " 'plot': (886, 225),\n",
       " 'response': (887, 225),\n",
       " '2000': (888, 224),\n",
       " 'musical': (889, 224),\n",
       " 'round': (890, 224),\n",
       " 'space': (891, 224),\n",
       " 'voice': (892, 224),\n",
       " 'wide': (893, 224),\n",
       " 'appear': (894, 223),\n",
       " 'crew': (895, 223),\n",
       " 'debut': (896, 223),\n",
       " 'groups': (897, 222),\n",
       " 'mounted': (898, 222),\n",
       " 'related': (899, 222),\n",
       " 'centre': (900, 221),\n",
       " 'jin': (901, 221),\n",
       " 'rachel': (902, 221),\n",
       " 'territory': (903, 221),\n",
       " 'view': (904, 221),\n",
       " 'billboard': (905, 220),\n",
       " 'ended': (906, 220),\n",
       " 'feature': (907, 220),\n",
       " 'films': (908, 220),\n",
       " 'nature': (909, 220),\n",
       " 'positive': (910, 220),\n",
       " 'saint': (911, 220),\n",
       " 'science': (912, 220),\n",
       " 'culture': (913, 219),\n",
       " 'finally': (914, 219),\n",
       " 'flight': (915, 219),\n",
       " 'score': (916, 219),\n",
       " 'squadron': (917, 219),\n",
       " 'supported': (918, 219),\n",
       " 'becoming': (919, 218),\n",
       " 'money': (920, 218),\n",
       " 'pressure': (921, 218),\n",
       " 'always': (922, 217),\n",
       " 'books': (923, 217),\n",
       " 'charles': (924, 217),\n",
       " 'provide': (925, 217),\n",
       " 'smaller': (926, 217),\n",
       " '1995': (927, 216),\n",
       " 'anti': (928, 216),\n",
       " 'discovered': (929, 216),\n",
       " 'private': (930, 216),\n",
       " 'shown': (931, 216),\n",
       " 'board': (932, 215),\n",
       " 'minutes': (933, 215),\n",
       " 'particular': (934, 215),\n",
       " 'shortly': (935, 215),\n",
       " 'defeated': (936, 214),\n",
       " 'difficult': (937, 214),\n",
       " 'experience': (938, 214),\n",
       " 'mass': (939, 214),\n",
       " 'nations': (940, 214),\n",
       " 'person': (941, 214),\n",
       " 'peter': (942, 214),\n",
       " 'temple': (943, 214),\n",
       " 'trade': (944, 214),\n",
       " 'big': (945, 213),\n",
       " 'staff': (946, 213),\n",
       " 'subsequently': (947, 213),\n",
       " 'surface': (948, 213),\n",
       " 'effects': (949, 212),\n",
       " 'japan': (950, 212),\n",
       " 'lack': (951, 212),\n",
       " 'living': (952, 212),\n",
       " 'press': (953, 212),\n",
       " 'upper': (954, 212),\n",
       " 'zealand': (955, 212),\n",
       " 'professional': (956, 211),\n",
       " 'word': (957, 211),\n",
       " 'fact': (958, 210),\n",
       " 'greater': (959, 210),\n",
       " 'material': (960, 210),\n",
       " 'tv': (961, 210),\n",
       " '60': (962, 209),\n",
       " 'problems': (963, 209),\n",
       " 'room': (964, 209),\n",
       " 'self': (965, 209),\n",
       " 'teams': (966, 209),\n",
       " 'bridge': (967, 208),\n",
       " 'collection': (968, 208),\n",
       " 'cut': (969, 208),\n",
       " 'gods': (970, 208),\n",
       " 'idea': (971, 208),\n",
       " 'intended': (972, 208),\n",
       " 'interest': (973, 208),\n",
       " 'key': (974, 208),\n",
       " 'largely': (975, 208),\n",
       " 'news': (976, 208),\n",
       " 'numbers': (977, 208),\n",
       " 'scientology': (978, 208),\n",
       " 'cover': (979, 207),\n",
       " 'haifa': (980, 207),\n",
       " 'metres': (981, 207),\n",
       " 'mostly': (982, 207),\n",
       " 'primary': (983, 207),\n",
       " 'themselves': (984, 207),\n",
       " '2016': (985, 206),\n",
       " 'captured': (986, 206),\n",
       " 'cm': (987, 206),\n",
       " 'got': (988, 206),\n",
       " 'likely': (989, 206),\n",
       " 'machine': (990, 206),\n",
       " 'mountain': (991, 206),\n",
       " 'referred': (992, 206),\n",
       " 'traffic': (993, 205),\n",
       " 'xenon': (994, 205),\n",
       " 'yet': (995, 205),\n",
       " 'contract': (996, 204),\n",
       " 'entered': (997, 204),\n",
       " 'location': (998, 204),\n",
       " 'tower': (999, 204),\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = Word2VecParams()\n",
    "train_iter, valid_iter = get_data()\n",
    "tokenizer = get_tokenizer(params.TOKENIZER)\n",
    "vocab = build_vocab(train_iter, tokenizer, params)\n",
    "skip_gram = SkipGrams(vocab=vocab, params=params, tokenizer=tokenizer)\n",
    "model = Model(vocab=vocab, params=params).to(params.DEVICE)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=.01)\n",
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4064/4064 [00:13<00:00, 293.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love: dark (0.299) horses (0.294) element (0.281) change (0.281) oldham (0.274) \n",
      "hurricane: focus (0.314) reports (0.282) bible (0.269) anthony (0.263) siege (0.262) \n",
      "military: lay (0.295) worst (0.291) kombat (0.268) san (0.265) archaeological (0.263) \n",
      "army: sent (0.327) von (0.291) such (0.289) flooding (0.28) basis (0.273) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [01:55<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/5\n",
      "     Train Loss: 2.5\n",
      "     Valid Loss: 0.51\n",
      "     Training Time (mins): 1.9\n",
      "love: change (0.385) dark (0.37) in (0.361) 40 (0.357) observed (0.356) \n",
      "hurricane: focus (0.333) episode (0.327) reports (0.317) bible (0.314) treatment (0.313) \n",
      "military: worst (0.38) san (0.333) scoring (0.329) where (0.324) existed (0.315) \n",
      "army: sent (0.396) such (0.391) flooding (0.359) basis (0.352) von (0.344) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 17/73 [00:17<00:57,  1.03s/it]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model, \n",
    "        params=params,\n",
    "        optimizer=optimizer, \n",
    "        train_iter=train_iter, \n",
    "        valid_iter=valid_iter, \n",
    "        vocab=vocab,\n",
    "        skipgrams=skip_gram\n",
    "    )\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
