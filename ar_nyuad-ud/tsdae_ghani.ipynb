{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new packages will be installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqq camel-tools\n",
    "!camel_data -i all\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أعْلَنَتِ الشَّرِكَةُ عَنْ بِضَاعَتِهَا بِوَاسِطَةِ إِعْلاَناتٍ تِجَارِيَّةٍ',\n",
       " 'أُنَاشِدُكُمْ أَنْ نَعْمَلَ كَأُمَّةٍ جُمِعَتْ كَلِمَتُهَا وَوُحِّدَتْ غَايَتُهَا',\n",
       " 'إِنَّ اللَّهَ لاَ يُغَيِّرُ مَا بِقَوْمٍ حَتَّى يُغَيِّرُوا مَا بِأَنْفُسِهِمْ',\n",
       " 'إِنَّ النّبِيَّ تَوَضَّأَ فَضَاقَ عَنْ يَدَيْهِ كُمَّا جِمَازَةٍ كَانَتْ عَلَيْه',\n",
       " 'إِنْ كَانَ لَسِناً سُمِّيَ مِهْذَاراً وإِنْ كَانَ صَمُوتاً سُمِّيَ عَيِيّاً']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "with open('ghani.txt', 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        line = line.strip()[1:-1]\n",
    "        corpus.append(line)\n",
    "corpus[910:915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw ['أَعْلَن/PV+َت/PVSUFF_SUBJ:3FS', 'ال/DET+شَرِك/NOUN+َة/NSUFF_FEM_SG+ِ/CASE_DEF_GEN', 'عَن/PREP', 'بِضاع/NOUN+َة/NSUFF_FEM_SG+ِ/CASE_DEF_GEN+ها/POSS_PRON_3FS', 'بِ/PREP+واسِط/NOUN+َة/NSUFF_FEM_SG+ِ/CASE_DEF_GEN', 'إِعْلان/NOUN+ات/NSUFF_FEM_PL+ٍ/CASE_INDEF_GEN', 'تِجارِيّ/ADJ+َة/NSUFF_FEM_SG+ٍ/CASE_INDEF_GEN']\n",
      "pos ['verb', 'noun', 'prep', 'noun', 'noun', 'noun', 'adj']\n",
      "atbtok ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَةِ_+ها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "atbseg ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَتِ_+ها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "bwtok ['أَعْلَن_+َت', 'ال+_شَرِك_+َة_+ِ', 'عَن', 'بِضاع_+َت_+ِ_+ها', 'بِ+_واسِط_+َة_+ِ', 'إِعْلان_+ات_+ٍ', 'تِجارِيّ_+َة_+ٍ']\n",
      "d1tok ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَتِها', 'بِواسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "d1seg ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَتِها', 'بِواسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "d2tok ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَتِها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "d2seg ['أَعْلَنَت', 'الشَّرِكَةِ', 'عَن', 'بِضاعَتِها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "d3tok ['أَعْلَنَت', 'ال+_شَرِكَةِ', 'عَن', 'بِضاعَةِ_+ها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "d3seg ['أَعْلَنَت', 'ال+_شَّرِكَةِ', 'عَن', 'بِضاعَتِ_+ها', 'بِ+_واسِطَةِ', 'إِعْلاناتٍ', 'تِجارِيَّةٍ']\n",
      "catib6 ['VRB', 'NOM', 'PRT', 'NOM+NOM', 'PRT+NOM', 'NOM', 'NOM']\n",
      "ud ['VERB', 'NOUN', 'ADP', 'NOUN+PRON', 'ADP+NOUN', 'NOUN', 'ADJ']\n"
     ]
    }
   ],
   "source": [
    "# mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "# msa_bw_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme='bwtok')\n",
    "# line_tok = msa_bw_tokenizer.tokenize(corpus[910].split())\n",
    "# print(line_tok)\n",
    "\n",
    "mled = MLEDisambiguator.pretrained()\n",
    "for feature in ['bw', 'pos',\n",
    "                'atbtok', 'atbseg', 'bwtok', 'd1tok', 'd1seg', 'd2tok', 'd2seg', 'd3tok', 'd3seg',\n",
    "                'catib6', 'ud',]:\n",
    "    tagger = DefaultTagger(mled, feature)\n",
    "    print(feature, tagger.tag(corpus[910].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70675/70675 [01:31<00:00, 770.75it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VERB', 'NOUN', 'ADP', 'NOUN+PRON', 'ADP+NOUN', 'NOUN', 'ADJ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = DefaultTagger(mled, 'ud')\n",
    "corpus_txfm = []\n",
    "for line in tqdm(corpus):\n",
    "    corpus_txfm.append(tagger.tag(line.split()))\n",
    "corpus_txfm[910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ghani_ud.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in corpus_txfm:\n",
    "        if None in line:\n",
    "            line_flat = ''\n",
    "        else:\n",
    "            line_flat = ' '.join(line).strip()\n",
    "        file.write(f\"{line_flat}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic TSDAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['أعْلَنَتِ الشَّرِكَةُ عَنْ بِضَاعَتِهَا بِوَاسِطَةِ إِعْلاَناتٍ تِجَارِيَّةٍ',\n",
       "  'أُنَاشِدُكُمْ أَنْ نَعْمَلَ كَأُمَّةٍ جُمِعَتْ كَلِمَتُهَا وَوُحِّدَتْ غَايَتُهَا',\n",
       "  'إِنَّ اللَّهَ لاَ يُغَيِّرُ مَا بِقَوْمٍ حَتَّى يُغَيِّرُوا مَا بِأَنْفُسِهِمْ',\n",
       "  'إِنَّ النّبِيَّ تَوَضَّأَ فَضَاقَ عَنْ يَدَيْهِ كُمَّا جِمَازَةٍ كَانَتْ عَلَيْه',\n",
       "  'إِنْ كَانَ لَسِناً سُمِّيَ مِهْذَاراً وإِنْ كَانَ صَمُوتاً سُمِّيَ عَيِيّاً'],\n",
       " ['VERB NOUN ADP NOUN+PRON ADP+NOUN NOUN ADJ',\n",
       "  'VERB+PRON SCONJ VERB ADP+NOUN VERB NOUN+PRON SCONJ+VERB NOUN+PRON',\n",
       "  'NOUN PROPN PART VERB PRON ADP+NOUN ADP VERB PRON ADP+NOUN+PRON',\n",
       "  'NOUN ADJ VERB CCONJ+VERB ADP NOUN+PRON CCONJ PROPN VERB ADP+PRON',\n",
       "  'NOUN VERB VERB VERB PROPN CCONJ+SCONJ VERB NOUN VERB VERB'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "with open('ghani.txt', 'r', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        corpus.append(line.strip()[1:-1])\n",
    "\n",
    "corpus_txfm = []\n",
    "with open('ghani_ud.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        corpus_txfm.append(line.strip())\n",
    "\n",
    "corpus[910:915], corpus_txfm[910:915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -Uqq sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_sentences = random.sample(corpus_txfm, 1000)\n",
    "\n",
    "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "    model, decoder_name_or_path=model_name, tie_encoder_decoder=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    weight_decay=0,\n",
    "    scheduler=\"constantlr\",\n",
    "    optimizer_params={\"lr\": 3e-5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(corpus_txfm[:10000])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لِكُلِّ إِنْسانٍ الحَقُّ في الوُجودِ والحُرِّيَّةِ والأمْنِ، والحَقُّ في حُرِّيَّةِ الرَّأْيِ والتَّعْبيرِ بِدونِ تَمْيِيزٍ بِسَبَبِ العُنْصُرِ أَوِ اللَّوْنِ أَوِ الجِنْسِ أَوِ الثَّقافَةِ أَوِ الدِّينِ أَوِ الرَّأْيِ\n",
      "طَائِرٌ مِنْ فَصيلَةِالوَزِّيَّاتِ، مِنْ رُتْبةِ الكَفِّيَّاتِ، لَهُ مِنْقارٌ عَريضٌ مُلَوَّنٌ، قَصيرُ العُنُقِ والرِّجْلَيْنِ، طَويلُ الأجْنِحَةِ، يَخْتَلِفُ عَنِ الإِوَزِّ، وَإِنْ كانَ يُشْبِهُهُ وَهُوَ طائِرٌ مائِيٌّ لَهُ قُدْرَةٌ على الطَّيَران\n",
      "يَبْلُغُ عَدَدُ حُروفِ هِجاءِ اللُّغَةِ العَرَبِيَّةِ ثَمانِيَةً وَعِشْرينَ حَرْفاً هي: ا ب ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ك ل م ن هـ و ي. وَهُوَ ما يُسَمَّى بِحُروفِ الْمَبانِ\n",
      "اِنْقَشَعَ السّحابُ» لا مَحَلَّ لها مِنَ الإِعْرابِ لأَنَّها جُمْلَةٌ اِبْتِدائِيَّةٌ، والجُمَلُ التي لها مَحَلٌّ مِنَ الإِعْرابِ، هي التي تَحَلُّ مَحَلَّ مُفْرَدٍ، أي ما لَيْس جُمْلَةً ولا شِبْهَ جُمْلَة\n",
      "جَمَاعَةٌ مِنَ النَّاسِ تَجْمَعُهُمْ رَوَابِطُ تَارِيخِيَّةٌ مُشْتَرَكَةٌ، قَدْ يَكُونُ فِيهَا مَا هُوَ لُغَوِيٌّ أوْ دِينِيٌّ أوِ اقْتِصَادِيٌّ وَلَهُمْ أهْدَافٌ مُشْتَرَكَةٌ فِي العَقِيدَةِ أَوِ السِّيَاسَةِ أَوِ الاقْتِصَاد\n",
      "لِكُلِّ إنْسَانٍ حَقُّ التَّمَتُّعِ بِكَافَّةِ الحُقُوقِ وَالحُرِّيَّاتِ دُونَ أيِّ تَمْيِيزٍ، كَالتَّمْيِيزِ بَيْنَ العُنْصُرِ أَوِ اللوْنِ أو الجِنْسِ أَوِ الدِّينِ أوِ الرَّأيِ السِّيَاسِيِّ أَوْ أَيِّ رَأيٍ آخَرَ\n",
      "آلةٌ هَوَائِيَّةٌ نُحَاسِيَّةٌ أوْ خَشَبِيَّةٌ عَلَى شَكْلِ أُسْطُوَانَةٍ فِي ثَلاَثَةِ أرْبَاعِهِ الأولَى، ثُمَّ يَتَحَوَّلُ إلَى شَكْلٍ مَخْرُوطٍ فِي أسْفَلِهِ، كَانَ العَرَبُ يَسْتَخْدِمُونَهُ أثْنَاءَ الحُرُوب\n",
      "اِتَّجَها مَعاً إِلى قَلْبِ الضَّريحِ، وَبَعْد لَمْسِ الغِطاءِ الأَخْضَرِ وَتَقْبيلِ الجَوانِبِ الأرْبَعَةِ جَلَسا يَنْظُرانِ وَيَتَأَمَّلانِ المُقْرِئِينَ وَهُمْ يُرَدِّدونَ آياتِ القُرْآنِ الكَريمِ\n",
      "حَيَوَانٌ مِنْ فَصِيلَةِ الكَلْبِيَّاتِ مِنْ رُتْبَةِ الضَّوَارِي، لَهُ خَطْمٌ قَصِيرٌ وَذَنَبٌ طَوِيلٌ، يَتَسَاقَطُ شَعْرُهُ مَرَّةً فِي كُلِّ سَنَةٍ، يُعْرَفُ فِي الحِكَايَاتِ وَالقِصَصِ بِخِدَاعِهِ وتَحَايُلِه\n",
      "مَرَضٌ عَصَبِيٌّ». وَمِنْهَا مَا هُوَ اجْتِمَاعِيٌّ، أَيْ مَا يُخْرِجُ الكَائِنَ الْحَيَّ عَنْ حَدِّ الاعْتِدَالِ مِنْ نِفَاقٍ أَوْ تَقْصِيرٍ فِي الأَمْرِ . فِي قُلُوبِهِمْ مَرَضٌ فَزَادَهُمُ اللَّهُ مَرَضا\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8991, 0.9109, 0.9152, 0.9601, 0.9634, 0.9151, 0.8692, 0.8825,\n",
       "         0.9161],\n",
       "        [0.8991, 1.0000, 0.9464, 0.9707, 0.9347, 0.9233, 0.9686, 0.9511, 0.9796,\n",
       "         0.9801],\n",
       "        [0.9109, 0.9464, 1.0000, 0.9400, 0.9134, 0.9407, 0.9498, 0.9529, 0.9460,\n",
       "         0.9559],\n",
       "        [0.9152, 0.9707, 0.9400, 1.0000, 0.9361, 0.9249, 0.9463, 0.9492, 0.9616,\n",
       "         0.9808],\n",
       "        [0.9601, 0.9347, 0.9134, 0.9361, 1.0000, 0.9659, 0.9381, 0.8835, 0.9099,\n",
       "         0.9405],\n",
       "        [0.9634, 0.9233, 0.9407, 0.9249, 0.9659, 1.0000, 0.9369, 0.8871, 0.9198,\n",
       "         0.9339],\n",
       "        [0.9151, 0.9686, 0.9498, 0.9463, 0.9381, 0.9369, 1.0000, 0.9141, 0.9660,\n",
       "         0.9618],\n",
       "        [0.8692, 0.9511, 0.9529, 0.9492, 0.8835, 0.8871, 0.9141, 1.0000, 0.9355,\n",
       "         0.9633],\n",
       "        [0.8825, 0.9796, 0.9460, 0.9616, 0.9099, 0.9198, 0.9660, 0.9355, 1.0000,\n",
       "         0.9667],\n",
       "        [0.9161, 0.9801, 0.9559, 0.9808, 0.9405, 0.9339, 0.9618, 0.9633, 0.9667,\n",
       "         1.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBG_SENT_COUNT = 10\n",
    "print('\\n'.join(corpus[:DBG_SENT_COUNT]))\n",
    "model.similarity(embeddings[:DBG_SENT_COUNT,:], embeddings[:DBG_SENT_COUNT,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# hdb = HDBSCAN(min_cluster_size=3, max_cluster_size=50, n_jobs=8)\n",
    "# hdb.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# clusters = defaultdict(lambda: [])\n",
    "# for sent, prob, label in zip(corpus, hdb.probabilities_, hdb.labels_):\n",
    "#     clusters[label].append((sent, prob))\n",
    "# clusters = {k: v for _, k, v in sorted([(len(v), k, v) for k, v in clusters.items()])}\n",
    "\n",
    "# with open('tsdae_ghani_ud.csv', 'w') as f:\n",
    "#     f.write(f\"CLUSTER_COUNT={len(clusters)}\\n\")\n",
    "#     f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "#     for k, v in clusters.items():\n",
    "#         f.write(f\"\\nCluster {k:03d}:\\n\")\n",
    "#         for s, p in v:\n",
    "#             f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = KMeans(n_clusters=int(len(embeddings)**0.5) * 4)\n",
    "# kmeans = kmeans.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# clusters = defaultdict(lambda: [])\n",
    "# for sent, label in zip(corpus, kmeans.labels_):\n",
    "#     clusters[label].append(sent)\n",
    "# clusters = {k: v for _, k, v in sorted([(len(v), k, v) for k, v in clusters.items()])}\n",
    "\n",
    "# with open('tsdae_kmeans_ghani_ud.csv', 'w') as f:\n",
    "#     f.write(f\"CLUSTER_COUNT={len(clusters)}\\n\")\n",
    "#     # f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "#     for k, v in clusters.items():\n",
    "#         f.write(f\"\\nCluster {k:03d}:\\n\")\n",
    "#         for s in v:\n",
    "#             f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "meanshift = MeanShift(bandwidth=2)\n",
    "meanshift = meanshift.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "clusters = defaultdict(lambda: [])\n",
    "for sent, label in zip(corpus, meanshift.labels_):\n",
    "    clusters[label].append(sent)\n",
    "clusters = {k: v for _, k, v in sorted([(-len(v), k, v) for k, v in clusters.items()])}\n",
    "\n",
    "with open('tsdae_meanshift_ghani_ud.csv', 'w') as f:\n",
    "    f.write(f\"CLUSTER_COUNT={len(clusters)}\\n\")\n",
    "    # f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "    for k, v in clusters.items():\n",
    "        f.write(f\"\\nCluster {k:03d}:\\n\")\n",
    "        for s in v:\n",
    "            f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import OPTICS\n",
    "# optics = OPTICS(min_samples=3)\n",
    "# optics = optics.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# clusters = defaultdict(lambda: [])\n",
    "# for sent, label in zip(corpus, optics.labels_):\n",
    "#     clusters[label].append(sent)\n",
    "# clusters = {k: v for _, k, v in sorted([(len(v), k, v) for k, v in clusters.items()])}\n",
    "\n",
    "# with open('tsdae_optics_ghani_ud.csv', 'w') as f:\n",
    "#     f.write(f\"CLUSTER_COUNT={len(clusters)}\\n\")\n",
    "#     f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "#     # f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "#     for k, v in clusters.items():\n",
    "#         f.write(f\"\\nCluster {k:03d}:\\n\")\n",
    "#         for s in v:\n",
    "#             f.write(f\"{s}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# affinity = AffinityPropagation(random_state=5)\n",
    "# affinity = affinity.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# clusters = defaultdict(lambda: [])\n",
    "# for sent, label in zip(corpus, affinity.labels_):\n",
    "#     clusters[label].append(sent)\n",
    "# clusters = {k: v for _, k, v in sorted([(-len(v), k, v) for k, v in clusters.items()])}\n",
    "\n",
    "# with open('tsdae_affinity_ghani_ud.csv', 'w') as f:\n",
    "#     f.write(f\"CLUSTER_COUNT={len(clusters)}\\n\")\n",
    "#     # f.write(f\"UNCLUSTERED= {len(clusters[-1]):,} / {len(embeddings):,}\\n\")\n",
    "#     for k, v in clusters.items():\n",
    "#         f.write(f\"\\nCluster {k:03d}:\\n\")\n",
    "#         for s in v:\n",
    "#             f.write(f\"{s}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
