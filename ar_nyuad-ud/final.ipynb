{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33,564 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ظَلَفَ نَفْسَهُ عَمَّا يَشينُها', 'N NR عمR VR'),\n",
       " ('الْمُقْتَرِعُ فِي الانْتِخَابِ', 'N في N'),\n",
       " ('مَالَهُ صَادِرٌ وَلاَ وَارِدٌ', 'NR J ولا J'),\n",
       " ('أَيُّهَا النَّائِمُونَ اِسْتَيْقِظُوا', 'أيها J V'),\n",
       " ('بَدَأَ لَوْحُ البَابِ يَتَشَظَّى', 'V V N V'),\n",
       " ('تَرَكَ أَمْرَهُ نَدَباً', 'V NR V'),\n",
       " ('ضَمِنَ عَلَى أَهْلِهِ', 'N على NR'),\n",
       " ('اِجْتَمَعَ أعْيانُ القَوْمِ فِي القَصْرِ البَلَدِيِّ', 'V N N في N J'),\n",
       " ('اِنْتَظَرَ آخِرَ لَحْظَةٍ لِيَشْتَرِيَ خَرُوفَ الْعِيدِ', 'V J N لV N N'),\n",
       " ('يَسْتَمْتِعُ بِالسِّباحَةِ عَلى الشَّاطِئِ', 'V بN على N')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv, random\n",
    "\n",
    "# SYNTH_METHOD = 'ud'\n",
    "# SYNTH_METHOD = 'UD'\n",
    "# SYNTH_METHOD = 'arud+literal'\n",
    "SYNTH_METHOD = 'ud+literal'\n",
    "# SYNTH_METHOD = 'catib6'\n",
    "# SYNTH_METHOD = 'pattern'\n",
    "# SYNTH_METHOD = 'reinflect'\n",
    "\n",
    "corpus_txfm = {}\n",
    "with open(f'final_corpus_{SYNTH_METHOD}.csv', 'r', encoding='utf-8') as feat:\n",
    "    reader = csv.reader(feat)\n",
    "    for row in reader:\n",
    "        corpus_txfm[row[0]] = row[1]\n",
    "\n",
    "print(f\"{len(corpus_txfm):,} sentences.\")\n",
    "\n",
    "corpus_txfm = list(corpus_txfm.items())\n",
    "random.shuffle(corpus_txfm)\n",
    "corpus_txfm = {k: v for k, v in corpus_txfm}\n",
    "\n",
    "list(corpus_txfm.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Semantic Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['أعْلَنَتِ الشَّرِكَةُ عَنْ بِضَاعَتِهَا بِوَاسِطَةِ إِعْلاَناتٍ تِجَارِيَّةٍ',\n",
       " 'أُنَاشِدُكُمْ أَنْ نَعْمَلَ كَأُمَّةٍ جُمِعَتْ كَلِمَتُهَا وَوُحِّدَتْ غَايَتُهَا',\n",
       " 'إِنَّ اللَّهَ لاَ يُغَيِّرُ مَا بِقَوْمٍ حَتَّى يُغَيِّرُوا مَا بِأَنْفُسِهِمْ',\n",
       " 'إِنَّ النّبِيَّ تَوَضَّأَ فَضَاقَ عَنْ يَدَيْهِ كُمَّا جِمَازَةٍ كَانَتْ عَلَيْه',\n",
       " 'إِنْ كَانَ لَسِناً سُمِّيَ مِهْذَاراً وإِنْ كَانَ صَمُوتاً سُمِّيَ عَيِيّاً']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# stats = []\n",
    "corpus = []\n",
    "with open('ghani.txt', 'r', encoding='utf-8') as feat:\n",
    "    for i, line in enumerate(feat):\n",
    "        line = line.strip()[1:-1]\n",
    "        # if len(line) == 0: continue\n",
    "        # toks = disamb.disambiguate(simple_word_tokenize(line))\n",
    "        # tok_count = 0\n",
    "        # poss = set()\n",
    "        # roots = set()\n",
    "        # for t in toks:\n",
    "        #     tok_count += 1\n",
    "        #     if len(t.analyses) == 0:\n",
    "        #         continue\n",
    "        #     tok_count += t.analyses[0].analysis['pos'].count('+')\n",
    "        #     roots.add(t.analyses[0].analysis['root'])\n",
    "        #     for pos in t.analyses[0].analysis['pos'].split('+'): poss.add(pos)\n",
    "\n",
    "        # if tok_count < 2: continue\n",
    "        # stats.append({\n",
    "        #     'chars': len(line),\n",
    "        #     'words': line.count(' ') + 1,\n",
    "        #     'tokens': tok_count,\n",
    "        #     'roots': len(roots),\n",
    "        #     'poss': len(poss),\n",
    "        # })\n",
    "        corpus.append(line)\n",
    "\n",
    "\n",
    "corpus[910:915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(stats).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(stats).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISAMBIG=[DisambiguatedWord(word='أوعر', analyses=[ScoredAnalysis(score=1.0, analysis={'diac': 'أَوْعَر', 'lex': 'أَوْعَر', 'bw': 'أَوْعَر/NOUN', 'gloss': 'rougher;roughest;more;most_rugged', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': '0', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'i', 'cas': 'u', 'enc0': '0', 'rat': 'n', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'أَوْعَر', 'caphi': '2_a_w_3_a_r', 'd1tok': 'أَوْعَر', 'd2tok': 'أَوْعَر', 'pos_logprob': -2.411992, 'd3tok': 'أَوْعَر', 'd2seg': 'أَوْعَر', 'pos_lex_logprob': -99.0, 'num': 's', 'ud': 'ADJ', 'gen': 'm', 'catib6': 'NOM', 'root': '#.ع.ر', 'bwtok': 'أَوْعَر', 'pattern': 'أَوْ2َ3', 'lex_logprob': -99.0, 'atbtok': 'أَوْعَر', 'atbseg': 'أَوْعَر', 'd1seg': 'أَوْعَر', 'stem': 'أَوْعَر', 'stemgloss': 'rougher;roughest;more;most_rugged', 'stemcat': 'Nel'}, diac='أَوْعَر', pos_lex_logprob=-99.0, lex_logprob=-99.0)]), DisambiguatedWord(word='الطريق', analyses=[ScoredAnalysis(score=1.0, analysis={'diac': 'الطَرِيقِ', 'lex': 'طَرِيق', 'bw': 'ال/DET+طَرِيق/NOUN+ِ/CASE_DEF_GEN', 'gloss': 'the+road;way+[def.gen.]', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'g', 'enc0': '0', 'rat': 'i', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_طَّرِيقِ', 'caphi': '2_a_t._t._a_r_ii_q_i', 'd1tok': 'الطَّرِيقِ', 'd2tok': 'الطَّرِيقِ', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_طَرِيقِ', 'd2seg': 'الطَّرِيقِ', 'pos_lex_logprob': -3.204651, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'ط.ر.ق', 'bwtok': 'ال+_طَرِيق_+ِ', 'pattern': 'ال1َ2ِي3ِ', 'lex_logprob': -3.204651, 'atbtok': 'الطَّرِيقِ', 'atbseg': 'الطَّرِيقِ', 'd1seg': 'الطَّرِيقِ', 'stem': 'طَرِيق', 'stemgloss': 'road;way', 'stemcat': 'Ndu'}, diac='الطَرِيقِ', pos_lex_logprob=-3.204651, lex_logprob=-3.204651)]), DisambiguatedWord(word='على', analyses=[ScoredAnalysis(score=1.0, analysis={'diac': 'عَلَى', 'lex': 'عَلَى', 'bw': 'عَلَى/PREP', 'gloss': 'on;above', 'pos': 'prep', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'na', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'na', 'cas': 'na', 'enc0': '0', 'rat': 'na', 'source': 'lex', 'form_gen': 'na', 'form_num': 'na', 'd3seg': 'عَلَى', 'caphi': '3_a_l_aa', 'd1tok': 'عَلَى', 'd2tok': 'عَلَى', 'pos_logprob': -1.002116, 'd3tok': 'عَلَى', 'd2seg': 'عَلَى', 'pos_lex_logprob': -1.819512, 'num': 'na', 'ud': 'ADP', 'gen': 'na', 'catib6': 'PRT', 'root': 'ع.ل.#', 'bwtok': 'عَلَى', 'pattern': '1َ2َى', 'lex_logprob': -1.819512, 'atbtok': 'عَلَى', 'atbseg': 'عَلَى', 'd1seg': 'عَلَى', 'stem': 'عَلَى', 'stemgloss': 'on;above', 'stemcat': 'FW-Wa'}, diac='عَلَى', pos_lex_logprob=-1.819512, lex_logprob=-1.819512)]), DisambiguatedWord(word='المسافر', analyses=[ScoredAnalysis(score=1.0, analysis={'diac': 'المُسافِر', 'lex': 'مُسافِر', 'bw': 'ال/DET+مُسافِر/NOUN', 'gloss': 'the+traveling;traveler;passenger', 'pos': 'noun', 'prc3': '0', 'prc2': '0', 'prc1': '0', 'prc0': 'Al_det', 'per': 'na', 'asp': 'na', 'vox': 'na', 'mod': 'na', 'stt': 'd', 'cas': 'u', 'enc0': '0', 'rat': 'r', 'source': 'lex', 'form_gen': 'm', 'form_num': 's', 'd3seg': 'ال+_مُسافِر', 'caphi': '2_a_l_m_u_s_aa_f_i_r', 'd1tok': 'المُسافِر', 'd2tok': 'المُسافِر', 'pos_logprob': -0.4344233, 'd3tok': 'ال+_مُسافِر', 'd2seg': 'المُسافِر', 'pos_lex_logprob': -4.446309, 'num': 's', 'ud': 'NOUN', 'gen': 'm', 'catib6': 'NOM', 'root': 'س.ف.ر', 'bwtok': 'ال+_مُسافِر', 'pattern': 'المُ1ا2ِ3', 'lex_logprob': -4.446309, 'atbtok': 'المُسافِر', 'atbseg': 'المُسافِر', 'd1seg': 'المُسافِر', 'stem': 'مُسافِر', 'stemgloss': 'traveling;traveler;passenger', 'stemcat': 'Nall'}, diac='المُسافِر', pos_lex_logprob=-4.446309, lex_logprob=-4.446309)])]\n",
      "\n",
      "['pos', 'catib6', 'ud']\n",
      "أوعر ['noun', 'NOM', 'ADJ']\n",
      "الطريق ['noun', 'NOM', 'NOUN']\n",
      "على ['prep', 'PRT', 'ADP']\n",
      "المسافر ['noun', 'NOM', 'NOUN']\n",
      "\n",
      "['root', 'pattern']\n",
      "أوعر ['#.ع.ر', 'أَوْ2َ3']\n",
      "الطريق ['ط.ر.ق', 'ال1َ2ِي3ِ']\n",
      "على ['ع.ل.#', '1َ2َى']\n",
      "المسافر ['س.ف.ر', 'المُ1ا2ِ3']\n",
      "\n",
      "['enc0', 'prc0', 'prc1', 'prc2', 'prc3']\n",
      "أوعر ['0', '0', '0', '0', '0']\n",
      "الطريق ['0', 'Al_det', '0', '0', '0']\n",
      "على ['0', 'na', '0', '0', '0']\n",
      "المسافر ['0', 'Al_det', '0', '0', '0']\n",
      "\n",
      "['form_gen', 'gen', 'form_num', 'num']\n",
      "أوعر ['m', 'm', 's', 's']\n",
      "الطريق ['m', 'm', 's', 's']\n",
      "على ['na', 'na', 'na', 'na']\n",
      "المسافر ['m', 'm', 's', 's']\n",
      "\n",
      "['asp', 'cas', 'per', 'mod', 'vox', 'rat']\n",
      "أوعر ['na', 'u', 'na', 'na', 'na', 'n']\n",
      "الطريق ['na', 'g', 'na', 'na', 'na', 'i']\n",
      "على ['na', 'na', 'na', 'na', 'na', 'na']\n",
      "المسافر ['na', 'u', 'na', 'na', 'na', 'r']\n",
      "\n",
      "فِعْلَ\n",
      "الفِعْلِ\n",
      "???\n",
      "الفِعْلِ\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.reinflector import Reinflector\n",
    "# from camel_tools.disambig.bert import BERTUnfactoredDisambiguator\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "if 'db' not in locals(): db = MorphologyDB.builtin_db(flags='rag')\n",
    "# TODO use bert instead\n",
    "if 'disamb' not in locals(): disamb = MLEDisambiguator.pretrained()\n",
    "# if 'disamb' not in locals(): disamb = BERTUnfactoredDisambiguator.pretrained(model_name='msa')\n",
    "\n",
    "reinflector = Reinflector(db)\n",
    "tokenizer = MorphologicalTokenizer(disambiguator=disamb, scheme='atbseg')\n",
    "tagger = DefaultTagger(disamb, 'pos')\n",
    "\n",
    "dbg_sent = disamb.disambiguate(simple_word_tokenize('صدقني لا أموال عندي فأبذله إليكم'))\n",
    "dbg_sent = disamb.disambiguate(simple_word_tokenize('أوعر الطريق على المسافر'))\n",
    "print(f\"DISAMBIG={dbg_sent}\")\n",
    "\n",
    "# 'lex_logprob', 'pos_logprob', 'pos_lex_logprob', 'd3seg', atbtok', \n",
    "# 'd3tok', 'stt', 'bw', 'd1tok', 'atbseg', 'd2seg', 'd2tok', 'bwtok', 'd1seg'\n",
    "# 'source', 'gloss', 'diac', 'lex', 'caphi'\n",
    "# 'pos', 'ud', 'catib6',\n",
    "# 'pattern', 'root',  \n",
    "# 'enc0', 'prc0', 'prc1', 'prc2', 'prc3', \n",
    "# 'asp', 'num', 'cas', 'form_num', 'per', 'gen', 'form_gen', 'mod', 'vox', 'rat', \n",
    "for fs in [\n",
    "    ['pos', 'catib6', 'ud'],\n",
    "    ['root', 'pattern'],\n",
    "    ['enc0', 'prc0', 'prc1', 'prc2', 'prc3'],\n",
    "    ['form_gen', 'gen', 'form_num', 'num'],\n",
    "    ['asp', 'cas', 'per', 'mod', 'vox', 'rat'],\n",
    "]:\n",
    "    print()\n",
    "    print(fs)\n",
    "    for w in dbg_sent:\n",
    "        anal = w.analyses[0].analysis\n",
    "        print(w.word, [anal[f] for f in fs])\n",
    "\n",
    "print()\n",
    "for w in dbg_sent:\n",
    "    lemma = 'فعل'\n",
    "    anal = w.analyses[0].analysis\n",
    "    feats = {}\n",
    "    for feat in anal.keys():\n",
    "        if feat in ['ud', 'enc0', 'prc0', 'prc1', 'prc2', 'prc3']:\n",
    "            # VERB = vox, asb\n",
    "            # NOUN = per\n",
    "            # rat, cas, pattern, form_gen, form_num, num, gen\n",
    "        # if f in['stem', 'stemgloss', 'stemcat']: continue\n",
    "            feats[feat] = anal[feat]\n",
    "    r = reinflector.reinflect(lemma, feats)\n",
    "    if len(r) == 0: print('???')\n",
    "    else: print(r[0]['diac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud+literal\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "إلي N عن N يا N N ف إن N A1 V1 N N N V1 ل N N N N N\n",
      "V1 N N N و N و N J1 من N و N N . و J1 لا V1\n",
      "ب N . V1 J1 إلى N5 N N N N في J1 N\n",
      "N N V1 N لا N N N . و N N N N لا N N J1\n",
      "و N N إلا ك N و N N . و N N إلا J1 N N و J1 N\n"
     ]
    }
   ],
   "source": [
    "TAG2LETTER = {\n",
    "    'PROPN': 'N',\n",
    "    'NOUN': 'N',\n",
    "    'PRON': 'N',\n",
    "    'NUM': 'N',\n",
    "    'DET': 'N5',\n",
    "    'VERB': 'V1',\n",
    "    'AUX': 'A1',\n",
    "    'ADP': 'A2',\n",
    "    'PART': 'A3',\n",
    "    'SCONJ': 'A5',\n",
    "    'CCONJ': 'A4',\n",
    "    'ADJ': 'J1',\n",
    "    'ADV': 'J2',\n",
    "    'PUNCT': '.',\n",
    "    'INTJ': 'W',\n",
    "    'X': 'X',\n",
    "}\n",
    "LETTER2TAG = {k: v for v, k in TAG2LETTER.items()}\n",
    "# assert(len(TAG2LETTER) == len(LETTER2TAG))\n",
    "\n",
    "LETTER2TAG2 = {\n",
    "    'العلم': 'PROPN',\n",
    "    'اسم': 'NOUN',\n",
    "    # 'هم': 'PRON',\n",
    "    'قام': 'VERB',\n",
    "    'كان': 'AUX',\n",
    "    'نعت': 'ADJ',\n",
    "    'حال': 'ADV',\n",
    "    # 'L': 'ADP',\n",
    "    # 'T': 'PART',\n",
    "    # '.': 'PUNCT',\n",
    "    'رقم': 'NUM',\n",
    "    # 'S': 'SCONJ',\n",
    "    # 'C': 'CCONJ',\n",
    "    # 'D': 'DET',\n",
    "    # 'I': 'INTJ',\n",
    "    'مجهول': 'X',\n",
    "}\n",
    "TAG2LETTER2 = {k: v for v, k in LETTER2TAG2.items()}\n",
    "assert(len(TAG2LETTER2) == len(LETTER2TAG2))\n",
    "\n",
    "if SYNTH_METHOD == 'catib6':\n",
    "    tagger = DefaultTagger(disamb, 'catib6')\n",
    "else:\n",
    "    tagger = DefaultTagger(disamb, 'ud')\n",
    "\n",
    "def synth_token(token, tag):\n",
    "    if SYNTH_METHOD in ['UD', 'catib6', 'pattern']:\n",
    "        return tag\n",
    "    if SYNTH_METHOD in ['ud+literal'] and tag in ['ADP', 'PART', 'SCONJ', 'CCONJ']:\n",
    "        return token.replace('_', '')\n",
    "    if SYNTH_METHOD == 'arud+literal':\n",
    "        if tag in ['ADP', 'PART', 'SCONJ', 'CCONJ', 'PUNCT', 'DET', 'PRON', 'INTJ']:\n",
    "            return token.replace('_', '')\n",
    "        else:\n",
    "            return TAG2LETTER2[tag]\n",
    "    return TAG2LETTER[tag]\n",
    "\n",
    "def synth_txt(txt, min_words=3):\n",
    "    # ignore short setences since they don't tell us much\n",
    "    if txt.count(' ') < min_words - 1: return None\n",
    "    txt = simple_word_tokenize(txt)\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    tags = tagger.tag(txt)\n",
    "    if '' in tags:\n",
    "        return None\n",
    "\n",
    "    joiner = ' '\n",
    "    if SYNTH_METHOD in ['UD', 'catib6']:\n",
    "        joiner = '+'\n",
    "\n",
    "    ans = []\n",
    "    if SYNTH_METHOD == 'pattern':\n",
    "        for token in disamb.disambiguate(txt):\n",
    "            if len(token.analyses) == 0: return\n",
    "            anal = token.analyses[0].analysis\n",
    "            ans.append(anal['pattern'])\n",
    "    else:\n",
    "        for token_agg, tag_agg in zip(tokens, tags):\n",
    "            if token_agg is None or tag_agg is None or token_agg.count('+') != tag_agg.count('+'):\n",
    "                return None\n",
    "            ans.append(joiner.join([synth_token(token, tag) for token, tag in zip(token_agg.split('+'), tag_agg.split('+'))]))\n",
    "    return ' '.join(ans)\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "\n",
    "print(synth_txt(\"بالله\", 1))\n",
    "print(synth_txt(\"بما\", 1))\n",
    "print(synth_txt(\"مازال\", 1))\n",
    "print(synth_txt(\"ثلاثة\", 1))\n",
    "print(synth_txt(\"إليك عني يا أخي فإنه قد تكون عندي هموم يطيش لها عقل ذي الحلم فرقا\", 1))\n",
    "print(synth_txt(\"منحني الله الصحة والعافية ونصيبا وافرا من العلم و 10 أبناء، والحسود لا يسود\", 1))\n",
    "print(synth_txt(\"بلى! نظرت طويلا إلى هؤلاء الأطفال تحت بيتنا في حر الصيف\", 1))\n",
    "\n",
    "print(synth_txt(\"عمر الفتى ذكره لا طول مدته، وموته خزيه لا يومه الآتي\", 1))\n",
    "print(synth_txt(\"وما المرء إلا كالهلال وضوئه، وما المرء إلا الأصغران لسانه ومعقوله\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70675/70675 [04:26<00:00, 265.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n",
      "37106/70675 FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('لاَ أَثِقُ بِحُكْمِ نَفْسِي حَتَّى يُؤَيِّدَ النَّاسُ ظَنِّي أَوْ يُكَذِّبُوهُ',\n",
       "  'لا قام ب اسم اسم ي حتى قام اسم اسم ي أو قام ه'),\n",
       " ('لاَ يَجُوزُ القَبْضُ عَلَى إِنْسَانٍ أَوْ حَجْزُهُ أَوْ نَفْيُهُ تَعَسُّفاً',\n",
       "  'لا قام اسم على اسم أو قام ه أو اسم ه اسم'),\n",
       " ('لاَ يَنَالُ الإنْسَانُ الغَايَةَ إلاَّ بِالجِدِّ وَسَهَرِ اللَّيَالِي',\n",
       "  'لا قام اسم اسم إلا ب اسم و قام اسم'),\n",
       " ('لَقَدْ حَقَّقَتِ البَشَرِيَّةُ تَقَدُّماً هائِلاً في القَرْنِ العِشْرِينَ',\n",
       "  'ل كان قام نعت اسم نعت في اسم اسم'),\n",
       " ('لَمْ يَكُنْ يَرَى فِي السُّهُولِ الَّتِي تَمُرُّ مِنْهَا إِلاَّ الْغَيْضَةَ',\n",
       "  'لم قام قام في اسم التي قام من ها إلا اسم')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_txfm = {}\n",
    "txfm_errors = []\n",
    "for sent in tqdm(corpus):\n",
    "    sent_txfm = synth_txt(sent)\n",
    "    if sent_txfm is None: txfm_errors.append(sent)\n",
    "    else: corpus_txfm[sent] = sent_txfm\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "print(f\"{len(txfm_errors)}/{len(corpus)} FAILED\")\n",
    "list(corpus_txfm.items())[910:915]\n",
    "# 5m8m 6m 2m2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n"
     ]
    }
   ],
   "source": [
    "print(SYNTH_METHOD)\n",
    "with open(f'final_corpus_{SYNTH_METHOD}.csv', 'w', encoding='utf-8') as feat:\n",
    "    writer = csv.writer(feat, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for sent, txfm in corpus_txfm.items():\n",
    "        writer.writerow([sent, txfm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n",
      "قام العلم اسم اسم عن اسم هم\n",
      "قام في اسم\n",
      "العلم اسم ب اسم\n",
      "قام اسم ب اسم\n",
      "قام في اسم نعت\n",
      "اسم اسم أو اسم\n",
      "قام اسم في اسم ه اسم و قام\n",
      "اسم في اسم نعت\n",
      "قام اسم اسم\n",
      "قام اسم نعت ه ب اسم\n",
      "768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f55590b6944f26a64fba617e6b92ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -Uqq sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "train_sentences = list(corpus_txfm.values())\n",
    "train_sentences = random.sample(train_sentences, 1000)\n",
    "print('\\n'.join(train_sentences[:10]))\n",
    "\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "# model_name = \"distilbert/distilroberta-base\"\n",
    "# model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-ca\"\n",
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sixteenth\"\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "embedding_len = word_embedding_model.get_word_embedding_dimension()\n",
    "print(embedding_len)\n",
    "\n",
    "pooling_model = models.Pooling(embedding_len, \"cls\")\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "train_loss = losses.DenoisingAutoEncoderLoss(\n",
    "    model, decoder_name_or_path=model_name, tie_encoder_decoder=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    weight_decay=0,\n",
    "    scheduler=\"constantlr\",\n",
    "    optimizer_params={\"lr\": 3e-5},\n",
    "    show_progress_bar=True,\n",
    "    output_path=f'ar_nyuad-ud/final_{SYNTH_METHOD}',\n",
    "    save_best_model=True,\n",
    ")\n",
    "# 1m30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud+literal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msid/fastai/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "print(SYNTH_METHOD)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(f\"ar_nyuad-ud/final_{SYNTH_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n",
      "(10000, 768)\n",
      "لِكُلِّ إِنْسانٍ الحَقُّ في الوُجودِ والحُرِّيَّةِ والأمْنِ، والحَقُّ في حُرِّيَّةِ الرَّأْيِ والتَّعْبيرِ بِدونِ تَمْيِيزٍ بِسَبَبِ العُنْصُرِ أَوِ اللَّوْنِ أَوِ الجِنْسِ أَوِ الثَّقافَةِ أَوِ الدِّينِ أَوِ الرَّأْيِ\n",
      "طَائِرٌ مِنْ فَصيلَةِالوَزِّيَّاتِ، مِنْ رُتْبةِ الكَفِّيَّاتِ، لَهُ مِنْقارٌ عَريضٌ مُلَوَّنٌ، قَصيرُ العُنُقِ والرِّجْلَيْنِ، طَويلُ الأجْنِحَةِ، يَخْتَلِفُ عَنِ الإِوَزِّ، وَإِنْ كانَ يُشْبِهُهُ وَهُوَ طائِرٌ مائِيٌّ لَهُ قُدْرَةٌ على الطَّيَران\n",
      "يَبْلُغُ عَدَدُ حُروفِ هِجاءِ اللُّغَةِ العَرَبِيَّةِ ثَمانِيَةً وَعِشْرينَ حَرْفاً هي: ا ب ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ك ل م ن هـ و ي. وَهُوَ ما يُسَمَّى بِحُروفِ الْمَبانِ\n",
      "اِنْقَشَعَ السّحابُ» لا مَحَلَّ لها مِنَ الإِعْرابِ لأَنَّها جُمْلَةٌ اِبْتِدائِيَّةٌ، والجُمَلُ التي لها مَحَلٌّ مِنَ الإِعْرابِ، هي التي تَحَلُّ مَحَلَّ مُفْرَدٍ، أي ما لَيْس جُمْلَةً ولا شِبْهَ جُمْلَة\n",
      "جَمَاعَةٌ مِنَ النَّاسِ تَجْمَعُهُمْ رَوَابِطُ تَارِيخِيَّةٌ مُشْتَرَكَةٌ، قَدْ يَكُونُ فِيهَا مَا هُوَ لُغَوِيٌّ أوْ دِينِيٌّ أوِ اقْتِصَادِيٌّ وَلَهُمْ أهْدَافٌ مُشْتَرَكَةٌ فِي العَقِيدَةِ أَوِ السِّيَاسَةِ أَوِ الاقْتِصَاد\n",
      "لِكُلِّ إنْسَانٍ حَقُّ التَّمَتُّعِ بِكَافَّةِ الحُقُوقِ وَالحُرِّيَّاتِ دُونَ أيِّ تَمْيِيزٍ، كَالتَّمْيِيزِ بَيْنَ العُنْصُرِ أَوِ اللوْنِ أو الجِنْسِ أَوِ الدِّينِ أوِ الرَّأيِ السِّيَاسِيِّ أَوْ أَيِّ رَأيٍ آخَرَ\n",
      "حَيَوَانٌ مِنْ فَصِيلَةِ الكَلْبِيَّاتِ مِنْ رُتْبَةِ الضَّوَارِي، لَهُ خَطْمٌ قَصِيرٌ وَذَنَبٌ طَوِيلٌ، يَتَسَاقَطُ شَعْرُهُ مَرَّةً فِي كُلِّ سَنَةٍ، يُعْرَفُ فِي الحِكَايَاتِ وَالقِصَصِ بِخِدَاعِهِ وتَحَايُلِه\n",
      "مَرَضٌ عَصَبِيٌّ». وَمِنْهَا مَا هُوَ اجْتِمَاعِيٌّ، أَيْ مَا يُخْرِجُ الكَائِنَ الْحَيَّ عَنْ حَدِّ الاعْتِدَالِ مِنْ نِفَاقٍ أَوْ تَقْصِيرٍ فِي الأَمْرِ . فِي قُلُوبِهِمْ مَرَضٌ فَزَادَهُمُ اللَّهُ مَرَضا\n",
      "مَقَامُ النَّغَمِ». هُوَ مَجْمُوعَةٌ سُلَّمِيَّةٌ مِنَ النَّغَمَاتِ الْمُتَتَابِعَةِ تُقَدَّرُ بِسَبْعِ نَغَمَاتٍ، وكُلُّ مَقَامٍ مُوسِيقِيٍّ لَهُ أَبْعَادٌ تَخْتَلِفُ عَنْ بَقِيَّةِ الْمَقَامَاتِ الأُخْرَ\n",
      "وأَحْسَنُ مَا تُوصَفُ بِهِ الكَرَامَةُ عَلَى مَذْهَبِهَا أَنَّهَا كِسْوَةٌ اجْتِمَاعِيَّةٌ ولاَ يَخْلَعُهَا الْمَرْءُ فِي الْمَجَالِسِ ولاَ يَلْبَسُهَا مُمَزَّقَةً أَوْ مُرَقَّعَةً أو مَوْصُومَةً\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8703, 0.9304, 0.8446, 0.8305, 0.9437, 0.9184, 0.8833, 0.8648,\n",
       "         0.8001],\n",
       "        [0.8703, 1.0000, 0.9276, 0.9123, 0.9417, 0.9337, 0.9602, 0.9475, 0.9488,\n",
       "         0.9083],\n",
       "        [0.9304, 0.9276, 1.0000, 0.8592, 0.8970, 0.9576, 0.9403, 0.9088, 0.9303,\n",
       "         0.8328],\n",
       "        [0.8446, 0.9123, 0.8592, 1.0000, 0.8382, 0.8589, 0.9131, 0.9260, 0.8600,\n",
       "         0.9531],\n",
       "        [0.8305, 0.9417, 0.8970, 0.8382, 1.0000, 0.9161, 0.9224, 0.9078, 0.9566,\n",
       "         0.8534],\n",
       "        [0.9437, 0.9337, 0.9576, 0.8589, 0.9161, 1.0000, 0.9554, 0.8899, 0.9464,\n",
       "         0.8321],\n",
       "        [0.9184, 0.9602, 0.9403, 0.9131, 0.9224, 0.9554, 1.0000, 0.9323, 0.9554,\n",
       "         0.8868],\n",
       "        [0.8833, 0.9475, 0.9088, 0.9260, 0.9078, 0.8899, 0.9323, 1.0000, 0.8960,\n",
       "         0.9163],\n",
       "        [0.8648, 0.9488, 0.9303, 0.8600, 0.9566, 0.9464, 0.9554, 0.8960, 1.0000,\n",
       "         0.8417],\n",
       "        [0.8001, 0.9083, 0.8328, 0.9531, 0.8534, 0.8321, 0.8868, 0.9163, 0.8417,\n",
       "         1.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "embeddings = model.encode(list(corpus_txfm.values())[:10_000])\n",
    "print(embeddings.shape)\n",
    "print('\\n'.join(list(corpus_txfm.keys())[:10]))\n",
    "model.similarity(embeddings[:10,:], embeddings[:10,:])\n",
    "# 9s 13s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "affinity = AffinityPropagation(random_state=5).fit(embeddings)\n",
    "# # 3m24s 4m10s 4m15s 3m50s 3m55s 3m55s 4m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "birch = Birch(n_clusters=None).fit(embeddings)\n",
    "# 8s 1s 1s 1s 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "hdb = HDBSCAN(min_cluster_size=3, max_cluster_size=50, n_jobs=8).fit(embeddings)\n",
    "# # 35s 32s 29s 30s 31s 35s 33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=int(len(embeddings)**0.5) * 8).fit(embeddings)\n",
    "# # 3m4s 6s 2s 7s 9s 8s 13s 14s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "meanshift = MeanShift(bandwidth=2).fit(embeddings)\n",
    "# # 1m35s 1m22s 1m46s 1m40s 1m50s 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "optics = OPTICS(min_samples=3).fit(embeddings)\n",
    "# # 5m20s 2m14s 2m20s 2m20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud+literal\n",
      "final_cluster_ud+literal_affinity.txt\n",
      "final_cluster_ud+literal_birch.txt\n",
      "final_cluster_ud+literal_hdbscan.txt\n",
      "final_cluster_ud+literal_kmeans.txt\n",
      "final_cluster_ud+literal_meanshift.txt\n",
      "final_cluster_ud+literal_optics.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{9: ['إِنْ كُنْتِ أَزْمَعْتِ الفِرَاقَ فَإِنَّمَا',\n",
       "  'فَإِذَا شَرَبْتُ فَإِنَّنِي مُسْتَهْلِكٌ',\n",
       "  'وَإِذَا ظُلِمْتُ فَإِنَّ ظُلْمِي بَاسِلٌ'],\n",
       " 5: ['تَطِسُ الإِكَامَ بِوَخْذِ خُفٍّ مِيْثَمِ',\n",
       "  'غَمَرَاتِهَا الأَبْطَالُ غَيْرَ تَغَمْغُمِ',\n",
       "  'وَسْطَ الدِّيَارِ تَسُفُّ حَبَّ الخِمْخِمِ'],\n",
       " 10: ['عَسِراً عَلَيَّ طِلَابُكِ ابْنَةَ مَخْرَمِ',\n",
       "  'قَدْحَ المُكِبِّ عَلَى الزِّنَادِ الأَجْذَمِ',\n",
       "  'لَيسَ الكَرِيمُ عَلَى القَنَا بِمُحَرَّمِ'],\n",
       " 2: ['مَالِي وَعِرْضِي وَافِرٌ لَم يُكْلَمِ',\n",
       "  'وَ مُدَجِجٍ كَرِهَ الكُمَاةُ نِزَالَهُ',\n",
       "  'وَأَبِيتُ فَوْقَ سَرَاةِ أَدْهَمَ مُلْجَمِ'],\n",
       " 7: ['حَشَّ الوَقُودُ بِهِ جَوَانِبَ قُمْقُمِ',\n",
       "  'غَضْبَى اتَّقَاهَا بِاليَدَينِ وَبِالفَمِ',\n",
       "  'هَزِجاً يَحُكُّ ذِرَاعَهُ بِذِرَاعِهِ',\n",
       "  'يُحْذَى نِعَالَ السِّبْتِ لَيْسَ بِتَوْأَمِ'],\n",
       " 4: ['سَحّاً وَتَسْكَاباً فَكُلَّ عَشِيِّةٍ',\n",
       "  'فَتَجَسَّسِي أَخْبَارَهَا لِيَ وَاعْلَمِي',\n",
       "  'فَتَرَكْتُهُ جَزَرَ السِّبَاعِ يَنُشْنَهُ',\n",
       "  'فَتَرَكْنَ كُلَّ قَرَارَةٍ كَالدِّرْهَمِ'],\n",
       " 6: ['صَعْلٍ يَعُودُ بِذِي العُشَيرَةِ بَيْضَةُ',\n",
       "  'مَا رَاعَنِي إِلَّا حَمُولَةُ أَهْلِهَا',\n",
       "  'نَهْدٍ مَرَاكِلُهُ نَبِيلِ المَحْزِمِ',\n",
       "  'يَتَذَامَرُونَ كَرَرْتُ غَيْرَ مُذَمَّمِ'],\n",
       " 11: ['إِذْ تَقْلِصُ الشَّفَتَانِ عَنْ وَضَحِ الفَمِ',\n",
       "  'بِالسَّيفِ عَنْ حَامِي الحَقِيقَةِ مُعْلِمِ',\n",
       "  'زَوْرَاءَ تَنْفِرُ عَن حِيَاضِ الدَّيْلَمِ',\n",
       "  'عَنْهَا وَلَاكِنِّي تَضَايَقَ مُقْدَمِي',\n",
       "  'وَ إِذَا صَحَوتُ فَمَا أُقَصِّرُ عَنْ نَداً'],\n",
       " 8: ['أَبْدَى نَوَاجِذَهُ لِغَيرِ تَبَسُّمِ',\n",
       "  'تَأْوِي لَهُ قُلُصُ النَّعَامِ كَمَا أَوَتْ',\n",
       "  'زَعْماً لَعَمْرُ أَبِيكِ لَيسَ بِمَزْعَمِ',\n",
       "  'لَمَّا رَءَانِي قَدْ نَزَلْتُ أُرِيدُهُ',\n",
       "  'لَمَّا رَأَيْتُ القَوْمَ أَقْبَلَ جَمْعُهُمْ',\n",
       "  'هِرٍّ جَنِيبٍ كُلَّمَا عَطَفَتْ لَهُ'],\n",
       " 3: ['أَغْشَى الوَغَى وَأَعِفُّ عِنْدَ المَغْنَمِ',\n",
       "  'تُمْسِي وَتُصْبِحُ فَوْقَ ظَهْرِ حَشِيَّةٍ',\n",
       "  'جَزَرَ السِّبَاعِ وكُلِّ نَسْرٍ قَشْعَمِ',\n",
       "  'خُضِبَ البَنَانُ وَرَأسُهُ بِالعَظْلَمِ',\n",
       "  'عُلِّقْتُهَا عَرَضاً وَأَقْتُلُ قَوْمَهَا',\n",
       "  'يَقْضِمْنَ حُسْنَ بَنَانِهِ وَالمِعْصَمِ'],\n",
       " 0: ['حُيِّيْتَ مِنْ طَلَلٍ تَقَادَمَ عَهْدُهُ',\n",
       "  'رَشَأٍ مِنَ الغِزْلَانِ حُرٍ أَرْثَمِ',\n",
       "  'قَالَتْ رَأَيتُ مِنَ الأَعَادِي غِرَّةً',\n",
       "  'هَلْ غادَرَ الشُّعَرَاءُ مِنْ مُتَرَدَّمِ',\n",
       "  'وَحْشِيِّ مِنْ هَزِجِ العَشِيِّ مُؤَوَّمِ',\n",
       "  'يَنْبَاعُ مِنْ ذِفْرَى غَضُوبٍ جَسْرَةٍ'],\n",
       " 1: ['خَطَّارَةٌ غِبَّ السُّرَى زَيَّافَةٌ',\n",
       "  'ذُلُلٌ رِكَابِي حَيْثُ شِئْتُ مُشَايِعِي',\n",
       "  'سُوداً كَخَافِيَةِ الغُرَابِ الأَسْحَمِ',\n",
       "  'عَذْبٍ مُقَبَّلُهُ لَذِيذِ المَطْعَمِ',\n",
       "  'كَالعَبْدِ ذِي الفَرْوِ الطَّوِيلِ الأَصْلَمِ',\n",
       "  'هَتَّاكِ غَايَاتِ التِّجَارِ مُلَوَّمِ'],\n",
       " 12: ['أَثْنِي عَلَيَّ بِمَا عَلِمْتِ فَإِنَّنِي',\n",
       "  'إِنْ كُنْتِ جَاهِلَةً بِمَا لَم تَعْلَمِي',\n",
       "  'طَوْراً يُجَرَّدُ لِلْطِعَانِ وَتَارَةً',\n",
       "  'فِيهَا اثْنَتَانِ وَأَرْبَعُونَ حَلُوبَةً',\n",
       "  'قِيْلُ الفَوَارِسِ وَيْكَ عَنْتَرَ أَقْدِمِ',\n",
       "  'مَازِلْتُ أَرْمِيهُمْ بِثُغْرَةِ نَحْرِهِ',\n",
       "  'هَلّا سَأَلْتِ الخَيلَ يَا ابْنَةَ مَالِكٍ'],\n",
       " -1: ['أَشْطَانُ بِئْرٍ فِي لَبَانِ الأَدْهَمِ',\n",
       "  'أَقْوَى وَأَقْفَرَ بَعْدَ أُمِّ الهَيْثَمِ',\n",
       "  'أَم هَلْ عَرَفْتَ الدَّارَ بَعْدَ تَوَهُّمِ',\n",
       "  'أَوْ رَوْضَةً أُنُفاً تَضَمَّنَ نَبْتَهَا',\n",
       "  'إِذْ تَسْتَبِيْكَ بِذِي غُرُوبٍ وَاضِحٍ',\n",
       "  'إِذْ لَا أَزَالُ عَلَى رِحَالَةِ سَابِحٍ',\n",
       "  'إِذْ يَتَّقُونَ بِيَ الأَسِنَّةَ لَم أَخِمْ',\n",
       "  'إِنْ تُغْدِفِي دُونِي القِنَاعَ فَإِنَّنِي',\n",
       "  'إِنْ يَفْعَلَا فَلَقَدْ تَرَكْتُ أَبَاهُمَا',\n",
       "  'بَرَكَتْ عَلَى جَنْبِ الرِّدَاعِ كَأَنَّمَا',\n",
       "  'بَرَكَتْ عَلَى قَصَبِ أَجَشَّ مُهَضَّمِ',\n",
       "  'بَطَلٍ كَأَنَّ ثِيَابَهُ فِي سَرْحَةٍ',\n",
       "  'بِالحَزْنِ فَالصَّمَّانِ فَالمُتَثَلَّمِ',\n",
       "  'بِزُجَاجَةٍ صَفْرَاءَ ذَاتِ أَسِرَّةٍ',\n",
       "  'بِعُنَيْزَتَيْنِ وأَهْلُنَا بِالغَيْلَمِ',\n",
       "  'بِقَرِيبِ بَينَ المَنْسِمَيْنِ مُصَلَّمِ',\n",
       "  'بِمُثَقَّفٍ صَدْقِ الكُعُوبِ مُقَوَّمِ',\n",
       "  'بِمُهَنَّدٍ صَافِي الحَدِيدَةِ مِخْذَمِ',\n",
       "  'تَمْكُو فَرِيصَتُهُ كَشَدْقِ الأَعْلَمِ',\n",
       "  'جَادَتْ عَلَيهِ كُلُّ بِكْرٍ حُرَّةٍ',\n",
       "  'جَادَتْ لَهُ كَفِّي بِعَاجِلِ طَعْنَةٍ',\n",
       "  'حَرَجٌ عَلَى نَعْشٍ لَهُنَّ مُخَيَّمِ',\n",
       "  'حَرُمَتْ عَلَيَّ وَلَيْتَهَا لَم تَحْرُمِ',\n",
       "  'حَلَّتْ بِأَرْضِ الزَّائِرِينَ فَأَصْبَحَتْ',\n",
       "  'حِزَقٌ يَمَانِيَةٌ لِأَعْجَمَ طِمْطِمِ',\n",
       "  'رَبِذٍ يَدَاهُ بِالقِدَاحِ إِذَا شَتَا',\n",
       "  'رَكَدَ الهَوَاجِرُ بِالمَشُوفِ المُعْلَمِ',\n",
       "  'زَيَّافَةٍ مِثْلَ الفَنِيقِ المُكْدَمِ',\n",
       "  'زُمَّت رَكَائِبُكُمْ بِلَيْلٍ مُظْلِمِ',\n",
       "  'سَبَقَتْ عَوَارِضَهَا إِلَيكَ مِنَ الفَمِ',\n",
       "  'سَبَقَتْ يَدَايَ لَهُ بِعَاجِلِ طَعْنَةٍ',\n",
       "  'سَمْحٌ مُخَالَقَتِي إِذَا لَم أُظْلَمِ',\n",
       "  'شَرِبَتْ بِمَاءِ الدُّحْرُضَينِ فَأَصْبَحَتْ',\n",
       "  'طَبٌّ بِأَخْذِ الفَارِسِ المُسْتَلْئِمِ',\n",
       "  'عَهْدِي بِهِ مَدَّ النَّهَارِ كَأَنَّمَا',\n",
       "  'غَرِداً كَفِعْلِ الشَّارِبِ المُتَرَنِّمِ',\n",
       "  'غَيْثٌ قَلِيلُ الدِّمْنِ لَيسَ بِمَعْلَمِ',\n",
       "  'فَازْوَرَّ مِنْ وَقْعِ القَنَا بِلِبَانِهِ',\n",
       "  'فَبَعَثْتُ جَارِيَتِي فَقُلْتُ لَهَا اذْهَبِي',\n",
       "  'فَدَنٌ لِأَقْضِيَ حَاجَةَ المُتَلَوِّمِ',\n",
       "  'فَشَكَكْتُ بِالرُّمْحِ الأَصَمِّ ثِيَابَهُ',\n",
       "  'فَطَعَنْتُهُ بِالرُّمْحِ ثُمَّ عَلَوْتُهُ',\n",
       "  'فَوَقَفْتُ فِيْهَا نَاقَتِي وَكَأَنَّهَا',\n",
       "  'فِي حَوْمَةِ الحَرْبِ الْلَتِي لَا تَشْتَكِي',\n",
       "  'قُرِنَتْ بِأَزْهَرَ فِي الشِّمَالِ مُقَدَّمِ',\n",
       "  'كَيفَ المَزَارُ وَقَد تَرَبَّعَ أَهْلُهَا',\n",
       "  'لَا مُمْعِنِ هَرَباً وَلَا مُسْتَسْلِمِ',\n",
       "  'لَو كَانَ يَدْرِي مَا المُحَاوَرَةُ اشْتَكَى',\n",
       "  'لُبِّي وَأَحْفِزُهُ بِأَمْرٍ مُبْرَمِ',\n",
       "  'لُعِنَتْ بِمَحْرُومِ الشَّرَابِ مُصَرَّمِ',\n",
       "  'لِلْحَرْبِ دَائِرَةٌ عَلَى ابْنَي ضَمْضَمِ',\n",
       "  'مُرٌّ مَذَاقَتُهُ كَطَعْمِ العَلْقَمِ',\n",
       "  'مِن بَيْنَ شَيْظَمَةِ وَءَاخَرَ شَيْظَمِ',\n",
       "  'مِنِّي بِمَنْزِلَةِ المُحِبِّ المُكْرَمِ',\n",
       "  'نَهْدِ تَعَاوُرُهُ الكُمَاةُ مُكَلَّمِ',\n",
       "  'نُبِّئْتُ عَمْراً غَيْرَ شَاكِرِ نِعْمَتِي',\n",
       "  'هَل تُبْلِغَنِّي دَارَهَا شَدَنِيَّةٌ',\n",
       "  'وَ حَلِيلِ غَانِيَةٍ تَرَكْتُ مُجَدَّلاً',\n",
       "  'وَ لَقَد شَرِبْتُ مِنَ المُدَامَةِ بَعْدَمَا',\n",
       "  'وَ لَقَدْ خَشَيْتُ بِأَنْ أَمُوتَ وَ لَم تَدُرْ',\n",
       "  'وَ لَقَدْ شَفَى نَفْسِي وَ أَذْهَبَ سُقْمَهَا',\n",
       "  'وَ لِبَانِهِ حَتَّى تَسَرْبَلَ بِالدَّمِ',\n",
       "  'وَالخَيلُ تَقْتَحِمُ الغُبَارَ عَوَابِساً',\n",
       "  'وَالشَّاةُ مُمْكِنَةٌ لِمَنْ هُو مُرْتَمِي',\n",
       "  'وَالكُفْرُ مَخْبَثَةٌ لِنَفْسِ المُنْعِمِ',\n",
       "  'وَتَحُلُّ عَبْلَةُ بِالجَوَاءِ وَأَهْلُنَا',\n",
       "  'وَحَشِيَّتِي سَرْجٌ عَلَى عَبْلِ الشَّوَى',\n",
       "  'وَخَلَى الذُّبَابُ بِهَا فَلَيسَ بِبَارِحٍ',\n",
       "  'وَرَشَاشِ نَافِذَةٍ كَلَوْنِ العَنْدَمِ',\n",
       "  'وَشَكَا إِلَىَّ بِعَبْرَةِ وَ تَحَمْحُمِ',\n",
       "  'وَعِمِي صَبَاحاً دَارَ عَبْلَةَ وَاسْلَمِي',\n",
       "  'وَكَأَنَّ رُبّاً أَوْ كُحَيْلاً مُقْعَداً',\n",
       "  'وَكَأَنَّ فَأْرَةَ تَاجِرٍ بِقَسِيْمَةٍ',\n",
       "  'وَكَأَنَّمَا الْتَفَتَتْ بِجِيدِ جَدَايَةٍ',\n",
       "  'وَكَأَنَّمَا تَطِسُ الإِكَامَ عَشِيَّةً',\n",
       "  'وَكَأَنَّمَا تَنْأَى بِجَانِبِ دَفِّهَل',\n",
       "  'وَكَمَا عَلِمْتِ شَمَائِلِي وَتَكَرُّمِي',\n",
       "  'وَلَقَد نَزَلْتِ فَلَا تَظُنِّيَ غَيْرَهُ',\n",
       "  'وَلَقَدْ حَفِظْتُ وَصَاةَ عَمِّي بِالضُّحَى',\n",
       "  'وَلَكَانَ لَو عَلِمَ الكَلَامَ مُكَلِّمِي',\n",
       "  'وَمِشَكِّ سَابِغَةٍ هَتَكْتُ فُرُوجَهَا',\n",
       "  'يَأْوِي إِلَى حَصِدِ القِسِيِّ عَرَمْرِمِ',\n",
       "  'يَا دَارَ عَبْلَةَ بِالجَوَاءِ تَكَلَّمِي',\n",
       "  'يَا شَاةَ مَا قَنَصٍ لِمَنْ حَلَّتْ لَهُ',\n",
       "  'يَتْبَعْنَ قُلَّةَ رَأْسِهِ وَكَأَنَّهُ',\n",
       "  'يَجْرِي عَلَيهَا المَاءُ لَم يَتَصَرَّمِ',\n",
       "  'يَدْعُونَ عَنْتَرَ وَالرِّمَاحُ كَأَنَّهَا',\n",
       "  'يُخْبِرْكِ مَنْ شَهَدَ الوَقِيعَةَ أَنَّنِي']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def save_clustering(algorithm, labels):\n",
    "    clusters = defaultdict(lambda: [])\n",
    "    for sent, label in zip(corpus2, labels):\n",
    "        clusters[label].append(sent)\n",
    "    \n",
    "    if -1 not in clusters:\n",
    "        clusters[-1] = []\n",
    "    for ss in clusters.values():\n",
    "        if len(ss) == 1:\n",
    "            clusters[-1].extend(ss)\n",
    "\n",
    "    clusters = {k: v for k, v in clusters.items() if len(v) > 1 or k == -1}\n",
    "    clusters = {k: v for _, v, k in sorted([(len(v), sorted(v), k) for k, v in clusters.items()])}\n",
    "    n_unclustered = len(clusters[-1])\n",
    "    \n",
    "    path = f'final_cluster_{SYNTH_METHOD}_{algorithm}.txt'\n",
    "    print(path)\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(f\"CLUSTER_COUNT = {len(clusters)}\\n\")\n",
    "        file.write(f\"UNCLUSTERED = {n_unclustered:,} / {sum(map(len, clusters.values())):,}\\n\")\n",
    "        for label, sents in clusters.items():\n",
    "            file.write(f\"\\nCLUSTER {label} ({len(sents):,} sentences)\\n\")\n",
    "            for sent in sents:\n",
    "                if label == -1:\n",
    "                    file.write(f\" xx {sent}\\n\")\n",
    "                else:\n",
    "                    file.write(f\"    {sent}\\n\")\n",
    "    return clusters\n",
    "\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "save_clustering(\"affinity\", affinity.labels_)\n",
    "save_clustering(\"birch\", birch.predict(embeddings))\n",
    "save_clustering(\"hdbscan\", hdb.labels_)\n",
    "save_clustering(\"kmeans\", kmeans.labels_)\n",
    "save_clustering(\"meanshift\", meanshift.labels_)\n",
    "save_clustering(\"optics\", optics.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n",
      "SYNTACTIC SIMILARITY\n",
      "لا يعرف كوعه من بوعه\n",
      "لا قام اسم ه من اسم ه\n",
      "(1, 768)\n",
      "torch.Size([1, 10000])\n",
      "\n",
      "97.38\n",
      "لاَ يَأتِيهِ البَاطِلُ مِنْ بَيْنَ يَدَيْهِ\n",
      "لا قام ه اسم من اسم اسم ه\n",
      "\n",
      "95.78\n",
      "لَمْ يَنْسَحِبْ مِنْ عَمَلِهِ مُصَادَفَةً\n",
      "لم قام من اسم ه اسم\n",
      "\n",
      "95.34\n",
      "لَمْ يَكُنْ مِنَ السَّهْلِ تَجْرِيدُهُ مِنَ السِّلاَحِ\n",
      "لم قام من اسم اسم ه من اسم\n",
      "\n",
      "95.31\n",
      "لاَ يَعْرِف يَدَ الشَّيْءِ مِنْ رِجْلِهِ\n",
      "لا قام اسم اسم من اسم ه\n",
      "\n",
      "95.3\n",
      "رَجُلٌ أَبْلَهُ لاَ يَعْرِفُ مَا يُخْرِجُ مِنْ أُمِّ دِمَاغِهِ\n",
      "اسم اسم ه لا قام ما قام من أم اسم ه\n",
      "\n",
      "94.68\n",
      "أَخْرَجَ فَجأةً سِلاَحَهُ مِنْ مِعْطَفِهِ\n",
      "قام ف اسم اسم ه من اسم ه\n",
      "\n",
      "94.36\n",
      "مِنْ كَثْرَةِ بُكائِهِ اِسْتَنْزَفَ دُموعَهُ\n",
      "من اسم اسم ه قام اسم ه\n",
      "\n",
      "94.29\n",
      "مَا حَمَلَتْهُ العُرُوسُ مِنْ بَائِنَةٍ يَفُوقُ مَا أَخَذَتْهُ مِنْ صَدَاقٍ\n",
      "ما اسم ه اسم من اسم ه قام ما قام ه من اسم\n",
      "\n",
      "94.25\n",
      "يُصيبُهُ الزَّعَلُ مِنْ عَمَلِهِ الرَّتيبِ\n",
      "قام ه اسم من اسم ه اسم\n",
      "\n",
      "94.09\n",
      "كُنْتُ أَسْمَعُ شَخِيرَهُ مِنَ الغُرْفَةِ الْمُجَاوِرَةِ\n",
      "قام قام اسم ه من اسم اسم\n",
      "\n",
      "93.98\n",
      "مَا زَالَ لَدَيْهِ مُتَّسَعٌ مِنَ الوَقْتِ\n",
      "ما قام اسم ه اسم من اسم\n",
      "\n",
      "93.97\n",
      "جَرَّدَتْهُ سُلُطَاتُ بِلاَدِهِ مِنْ حُقُوقِهِ\n",
      "قام ه اسم اسم ه من اسم ه\n",
      "\n",
      "93.93\n",
      "فَاجَأَهُ وَهُوَ يَنْظُرُ مِنْ خَصَاصَةِ البَابِ\n",
      "قام ه و هو قام من اسم ه اسم\n",
      "\n",
      "93.89\n",
      "مَنَعَهُ تَعَذُّرُ الظُّرُوفِ مِنَ السَّفَرِ\n",
      "اسم ه قام اسم من اسم\n",
      "\n",
      "93.83\n",
      "لاَ يَنْجُو مِنْهُ أَحَدٌ إلاَّ مَنْ رَجَحَ حِلْمُهُ وَعَظُمَتْ مُرُوءتُهُ\n",
      "لا قام من ه اسم إلا من قام اسم ه و قام اسم ه\n",
      "\n",
      "93.72\n",
      "الاِسْتِبْدَادُ يَرْتَجِفُ مِنْ صَوْلَةِ الْعِلْمِ\n",
      "اسم قام من اسم ه اسم\n",
      "\n",
      "93.63\n",
      "لَيْسَ مِنْ حَقِّهِ أَنْ يُقَيِّدَ حُرِّيَّاتِ النَّاسِ\n",
      "قام من اسم ه أن قام اسم اسم\n",
      "\n",
      "93.63\n",
      "لَيْسَ مِنْ عَادَتِهِ أَنْ يَنْسَى وَقْتَ الصَّلاَةِ\n",
      "قام من اسم ه أن قام اسم اسم\n",
      "\n",
      "93.5\n",
      "لاَ يَعْرِفُ الوَجَلُ إِلَى قَلْبِهِ سَبِيلاً\n",
      "لا قام اسم إلى اسم ه اسم\n",
      "\n",
      "93.47\n",
      "اِجْتَمَعَتْ حَوْلَهُ دَارَةٌ مِنَ الجُمْهُورِ\n",
      "قام اسم ه اسم ه من اسم\n",
      "\n",
      "93.41\n",
      "مَنِ اتَّكَلَ عَلَى غَيْرِ نَفْسِهِ طَالَ جُوعُهُ\n",
      "من قام على اسم اسم ه قام اسم ه\n",
      "\n",
      "93.3\n",
      "لَمْ يَكُنْ رَاغِباً فِي مُسَاوَفَةِ دَائِنِهِ\n",
      "لم قام اسم في اسم ه اسم ه\n",
      "\n",
      "93.26\n",
      "كُلَّمَا وَقَعَ كِتَابٌ فِي يَدِهِ اِغْتَرَفَ مِنْهُ\n",
      "اسم قام اسم في اسم ه قام من ه\n",
      "\n",
      "93.21\n",
      "كَانَتْ كُلُّ مُقْتَنَيَاتِهِ مِنَ كَذَا\n",
      "قام اسم اسم ه من اسم\n",
      "\n",
      "93.21\n",
      "لاَحَظَ اتِّسَاعَ عَيْنَيْهِ مِنَ الدَّهْشَةِ\n",
      "قام اسم اسم ه من اسم\n",
      "\n",
      "93.21\n",
      "يَسْتَوْحِي الشَّاعِرُ شِعْرَهُ مِنَ الطَّبِيعَةِ\n",
      "قام اسم اسم ه من اسم\n",
      "\n",
      "93.21\n",
      "وَصَلَتْ أَعْدَادٌ غَفِيرَةٌ مِنَ النَّاسِ\n",
      "قام اسم اسم ه من اسم\n",
      "\n",
      "92.82\n",
      "طَلَبَ مِنْهُ إِقْراضَهُ مَبْلَغاً مِنَ المالِ\n",
      "قام من ه اسم ه اسم من اسم\n",
      "\n",
      "92.75\n",
      "مِنْ عادَتِهِ أن يُصْلِحَ بَيْنَ النَّاسِ\n",
      "من اسم ه أن قام اسم اسم\n",
      "\n",
      "92.75\n",
      "مِنْ عَادَتِهِ أنْ يَحْكُمَ بَيْنَ الْمُتَنَازِعِينَ\n",
      "من اسم ه أن قام اسم اسم\n",
      "\n",
      "92.75\n",
      "مِنْ عَادَتِهِ أَنْ يَجْبُرَ خَوَاطِرَ النَّاسِ\n",
      "من اسم ه أن قام اسم اسم\n",
      "\n",
      "92.75\n",
      "مِنْ عَادَتِهِ أَنْ يُؤَلِّفَ بَيْنَ النَّاسِ\n",
      "من اسم ه أن قام اسم اسم\n",
      "\n",
      "92.57\n",
      "اِسْتَغْرَقَ عَمَلُهُ بُرْهَةً مِنَ الزَّمَنِ\n",
      "قام اسم ه اسم من اسم\n",
      "\n",
      "92.57\n",
      "طَلَبَ إِمْهالَهُ فَتْرَةً مِنَ الوَقْتِ\n",
      "قام اسم ه اسم من اسم\n",
      "\n",
      "92.57\n",
      "كَانَتْ حَيَاتُهُ سِلْسِلَةً مِنَ الْمُفَاجَآتِ\n",
      "قام اسم ه اسم من اسم\n",
      "\n",
      "92.54\n",
      "اِسْتَجَابَ لِرَغَبَاتِهِ خِشْيَةً مِنْهُ\n",
      "قام ل اسم ه اسم من ه\n",
      "\n",
      "92.53\n",
      "مَنْ يَبْني دُونَ أُسُسٍ كَأَنَّهُ يَبْنِي فَوْقَ الرَّمْلِ\n",
      "من قام اسم اسم كأن ه قام اسم اسم\n",
      "\n",
      "92.46\n",
      "مِنْ عادَتِهِ أَنْ يُشْعِلَ الفِتْنَةَ بَيْنَ أَصْدِقائِهِ\n",
      "من اسم ه أن قام اسم اسم اسم ه\n",
      "\n",
      "92.45\n",
      "لَمْ يَكُنْ تَسَايُرُ الغَضَبِ عَنْ وَجْهِهِ سَهْلاً\n",
      "لم قام قام اسم عن اسم ه اسم\n",
      "\n",
      "92.44\n",
      "ذَكَرَ لَهُ واقِعَةً مِنْ أَيَّامِ طُفولَتِهِ\n",
      "قام ل ه اسم من اسم اسم ه\n",
      "\n",
      "92.42\n",
      "لَمْ يتَوَقَّف سُعَالُهُ طُولَ اللَّيْلِ\n",
      "لم قام اسم ه اسم اسم\n",
      "\n",
      "92.42\n",
      "لَمْ يَتَمالَكْ نَفْسَهُ لَحْظَةَ الانْدِفاعِ\n",
      "لم قام اسم ه اسم اسم\n",
      "\n",
      "92.37\n",
      "لاَ تُحَاوِلْ إِزْعَاجَهُ وَقْتَ العَمَلِ\n",
      "لا قام اسم ه اسم اسم\n",
      "\n",
      "92.34\n",
      "وَثَبَ إِلَى رَأْسِهِ سُؤَالٌ لاَ يَخْلُو مِنْ قَلَقٍ\n",
      "اسم إلى اسم ه اسم لا قام من اسم\n",
      "\n",
      "92.23\n",
      "ما أتَى بِهِ صَدِيقُهُ مِنْ أفْعالٍ أفْقَدَهُ الصَّوابَ\n",
      "ما قام ب ه اسم ه من اسم قام ه اسم\n",
      "\n",
      "92.13\n",
      "قَدَّمَ اسْتِقَالَتَهُ مِنْ مَكْتَبِ الْجَمْعِيَّةِ\n",
      "قام اسم ه من اسم اسم\n",
      "\n",
      "92.13\n",
      "اِقْتَبَسَ أفْكَارَهُ مِنْ كتابِ كَذا\n",
      "قام اسم ه من اسم اسم\n",
      "\n",
      "92.13\n",
      "اِصْطَكَّتْ رُكْبَتاهُ مِنْ كَثْرَةِ الْمَشْيِ\n",
      "قام اسم ه من اسم اسم\n",
      "\n",
      "92.13\n",
      "في حالَةِ غِيابِهِ سَأَجْلِسُ مَكانَهُ\n",
      "في اسم اسم ه كان قام اسم ه\n",
      "\n",
      "92.12\n",
      "طالَ طَرِيقُهُ كَمَا لَمْ تَطُلْ بِهِ طَرِيقٌ مِنْ قَبْلُ\n",
      "قام اسم ه كما لم قام ب ه اسم من اسم\n",
      "\n",
      "92.11\n",
      "قَلَّبَ الْمَوْضُوعَ مِنْ جَمِيعِ أَوْجُهِهِ\n",
      "اسم اسم من اسم قام ه\n",
      "\n",
      "92.05\n",
      "اِسْتَفَادَ فَائِدَةً عُظْمَى مِنْ عِلْمِهِ\n",
      "قام اسم اسم من اسم ه\n",
      "\n",
      "92.05\n",
      "أفَادَ الرَّجُلُ مَالاً مِنْ تِجَارَتِهِ\n",
      "قام اسم اسم من اسم ه\n",
      "\n",
      "92.05\n",
      "شَدَا الشَّاعِرُ قَصِيدَةً مِنْ شِعْرِهِ\n",
      "قام اسم اسم من اسم ه\n",
      "\n",
      "92.05\n",
      "مَنَعَهُ مَنْعاً باتّاً مِنْ دُخولِ الفَصْلِ\n",
      "اسم ه اسم قام من اسم اسم\n",
      "\n",
      "92.04\n",
      "عَذَرَهُ عَلَى مَا صَدَرَ مِنْهُ مِنْ قَوْلٍ\n",
      "اسم ه على ما قام من ه من اسم\n",
      "\n",
      "92.04\n",
      "أَصْبَحَتْ مِنْ بَعْدهِ لاَ تَمْلِكُ مَالاً وَلا عَضُداً\n",
      "قام من اسم ه لا قام اسم و لا اسم\n",
      "\n",
      "92.03\n",
      "آرَاؤُهُ تُعَبِّرُ عَنْ مَحْدُودِيَّةِ تَفْكِيرِهِ\n",
      "اسم ه قام عن اسم اسم ه\n",
      "\n",
      "92.0\n",
      "أَضَاعَ مالَهُ في شِراءِ ما لاَ فائِدَةَ مِنْهُ\n",
      "قام اسم ه في اسم ما لا اسم من ه\n",
      "\n",
      "91.99\n",
      "أَظْهَرَ شَرَهاً مَا بَعْدَهُ مِنْ شَرَهٍ\n",
      "قام اسم ها ما اسم ه من اسم ه\n",
      "\n",
      "91.94\n",
      "لَمْ يَكُنْ قَوْلُهُ فِي مُسْتَوَى فِعْلِهِ\n",
      "لم قام اسم ه في اسم اسم ه\n",
      "\n",
      "91.92\n",
      "كَانَتْ أَسَارِيرُ وَجْهِهِ تُعَبِّرُ عَنِ الاِرْتِيَاحِ\n",
      "قام اسم اسم ه قام عن اسم\n",
      "\n",
      "91.92\n",
      "اِزْدَحَمَتْ غَمَارَةٌ مِنَ النَّاسِ أَمَامَ دُكَّانِهِ\n",
      "قام اسم ه من اسم اسم اسم ه\n",
      "\n",
      "91.88\n",
      "اِسْتَوْفَى الْمَوْضوعَ مِنْ جَمِيعِ جَوانِبِهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "اِنْفَلَتَ السَّجِينُ مِنْ يَدِ حُرَّاسِهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "اِسْتَقْصى الْمَوْضوعَ مِنْ كُلِّ جَوانِبِهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "تَكَوَّنَتْ حَلْقَةٌ مِنَ الجُمْهُورِ حَوْلَهُ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "اِخْتَطَفَ اللَّحْمَ مِنْ بَيْنِ يَدَيْهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "تَمَلُّصُ السَّمَكَةِ مِنْ بَيْنِ أصَابِعِهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.88\n",
      "تَمَلَّصَتِ السَّمَكَةُ مِنْ بَيْنِ أصَابِعِهِ\n",
      "قام اسم من اسم اسم ه\n",
      "\n",
      "91.85\n",
      "كَانَ تَنَشُّطُهُ الطَّرِيقَ تَعْبِيراً عَنْ نَجَاحِهِ\n",
      "قام قام ه اسم اسم عن اسم ه\n",
      "\n",
      "91.84\n",
      "يَتَسَخَّرُهُ فِي كُلِّ عَمَلٍ مِنْ أَعْمَالِهِ\n",
      "قام ه في اسم اسم من اسم ه\n",
      "\n",
      "91.81\n",
      "تَصَرُّفَاتُهُ تَجْعَلُهُ يَقَعُ تَحْتَ طَائِلَةِ القَانُونِ\n",
      "اسم ه قام ه قام اسم اسم اسم\n",
      "\n",
      "91.74\n",
      "لاَ أَحَدَ يَعْرِفُ سَبَبَ انْتِحارِهِ\n",
      "لا اسم قام اسم اسم ه\n",
      "\n",
      "91.65\n",
      "يَتَخَلَّصُ شَيْئاً فَشَيْئاً مِنْ دُيُونِهِ\n",
      "قام اسم ف اسم من اسم ه\n",
      "\n",
      "91.62\n",
      "أَدْرَكَ الرَّجُلُ أَنَّ الْمَرْأَةَ إِنْسَانٌ مِنْ نَوْعِهِ\n",
      "قام اسم أن اسم اسم من اسم ه\n",
      "\n",
      "91.62\n",
      "أَخْرَجَ مِنْ جَيْبِهِ سِكِّيناً حادّاً\n",
      "قام من اسم ه اسم اسم\n",
      "\n",
      "91.62\n",
      "لاَ أَعْرِفُ سَبَبَ جُمُودِ عَاطِفَتِهِ\n",
      "لا قام اسم اسم اسم ه\n",
      "\n",
      "91.61\n",
      "طَلَبَ مِنْهُ التَّشْفِيعَ عِنْدَ رَئِيسِهِ\n",
      "قام من ه العلم اسم اسم ه\n",
      "\n",
      "91.58\n",
      "حَذَّرَهُ مِنَ الاِسْتِمْرارِ في اللَّهْوِ\n",
      "قام ه من اسم في اسم\n",
      "\n",
      "91.58\n",
      "نَبَّهَهُ مِنَ التَّوَغُّلِ فِي الغَابَةِ\n",
      "قام ه من اسم في اسم\n",
      "\n",
      "91.58\n",
      "أَعْطَى مِنْ مَالِهِ مِنْ بَابِ الفَضْلِ\n",
      "قام من اسم ه من اسم اسم\n",
      "\n",
      "91.56\n",
      "جَرَدَ الفَارِسُ سَيْفَهُ مِنْ غِمْدِهِ\n",
      "اسم العلم اسم ه من قام ه\n",
      "\n",
      "91.54\n",
      "تَغَاضَى عَنْ أَفْعَالِهِ وَلَمْ يُعِرْهُ اهْتِمَاماً\n",
      "اسم عن اسم ه و لم قام ه اسم\n",
      "\n",
      "91.5\n",
      "لاَ حَقَّ لَهُ في إِسْكاتِهِ عِنْدَمَا يُعَبِّرُ عَنْ رَأْيِهِ\n",
      "لا اسم ل ه في اسم ه اسم ما قام عن اسم ه\n",
      "\n",
      "91.47\n",
      "أَقْدَمَ على إِنْقاذِهِ مِنَ الخَطَرِ\n",
      "قام على اسم ه من اسم\n",
      "\n",
      "91.46\n",
      "إِذَا زادَ الشَّيْءُ عَنْ حَدِّهِ اِنْقَلَبَ إلى ضِدِّهِ\n",
      "إذا قام اسم عن اسم ه قام إلى اسم ه\n",
      "\n",
      "91.45\n",
      "قَدَّمَ لَهُ أَلْوَاناً مِنَ الطَّعَامِ\n",
      "قام ل ه اسم من اسم\n",
      "\n",
      "91.45\n",
      "تَعَرَّضَتْ لَهُ حَفْنَةٌ مِنَ الصَّعَالِيكِ\n",
      "قام ل ه اسم من اسم\n",
      "\n",
      "91.43\n",
      "لاَ يُفَارِقُهُ المِسْوَاكُ صَبَاحَ مَسَاءَ\n",
      "لا قام ه اسم اسم اسم\n",
      "\n",
      "91.42\n",
      "اِبْتِكَارَاتُهُ جَعَلَتْهُ مَوْضِعَ إِعْجَابٍ\n",
      "اسم ه قام ه اسم اسم\n",
      "\n",
      "91.4\n",
      "كانَ انْسِحابُهُ مِنَ الحَفْلَةِ في غَيْرِ مَحَلِّهِ\n",
      "قام اسم ه من اسم في اسم اسم ه\n",
      "\n",
      "91.38\n",
      "اِسْتَنْبَطَ مِنْهُ عِلْماً أَوْ مالاً\n",
      "قام من ه اسم أو اسم\n",
      "\n",
      "91.35\n",
      "لاَ تَنْهَ عَنْ خُلُقٍ وَتَأْتِيَ مِثْلَهُ\n",
      "لا قام عن اسم و قام اسم ه\n",
      "\n",
      "91.23\n",
      "هَزَّ الغُصْنَ فَنَدَرَتْ مِنْهُ الثِّمَارُ\n",
      "اسم اسم ف قام من ه اسم\n",
      "\n",
      "91.23\n",
      "أخْجَلَهُ لِمَا بَدَرَ مِنْهُ مِنْ أَفْعَالٍ سَيِّئةٍ\n",
      "قام ه ل ما اسم من ه من اسم اسم\n",
      "\n",
      "91.21\n",
      "طَلَبَ مِنْهُ عَدَمَ التَّفْرِيطِ فِي عَمَلِهِ\n",
      "قام من ه اسم اسم في اسم ه\n",
      "\n",
      "91.21\n",
      "طَلَبَ مِنْهُ فَسْحَ الْمَجَالِ لِيُعَبِّرَ عَنْ رَأْيِهِ\n",
      "قام من ه اسم اسم ل قام عن اسم ه\n",
      "\n",
      "91.16\n",
      "اِسْتَضافَ أَصْدِقاءهُ وَما حاشَى مِنْهُمْ أَحَداً\n",
      "قام اسم ه و ما قام من هم اسم\n",
      "\n",
      "91.14\n",
      "يَحْرِزُ الطَّالِبُ مِنْ مُجالَسَةِ صاحِبِ السُّوءِ\n",
      "قام اسم من اسم ه اسم اسم\n"
     ]
    }
   ],
   "source": [
    "print(SYNTH_METHOD)\n",
    "print(\"SYNTACTIC SIMILARITY\")\n",
    "\n",
    "\n",
    "dbg_txt = \"\"\"\n",
    "لا يعرف كوعه من بوعه\n",
    "\"\"\".strip()\n",
    "print(dbg_txt)\n",
    "dbg_txt = synth_txt(dbg_txt)\n",
    "print(dbg_txt)\n",
    "dbg_txt = model.encode([dbg_txt])\n",
    "print(dbg_txt.shape)\n",
    "\n",
    "import torch\n",
    "sim = model.similarity(dbg_txt, embeddings)\n",
    "print(sim.shape)\n",
    "top = torch.topk(sim, 100)\n",
    "top = [(i, v) for i, v in zip(top.indices[0], top.values[0]) if v.item() > .9]\n",
    "\n",
    "for dbg_index, dbg_val in top:\n",
    "    print()\n",
    "    dbg_match = list(corpus_txfm.items())[dbg_index]\n",
    "    print(dbg_val.item() * 1000000 // 100 / 100)\n",
    "    print(dbg_match[0])\n",
    "    print(dbg_match[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEMANTIC SIMILARITY\n",
      "arud+literal\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEMANTIC SIMILARITY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(SYNTH_METHOD)\n\u001b[0;32m----> 7\u001b[0m dbg_embeds_sem \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_sem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbg_txt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(dbg_embeds_sem\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m sim_sem \u001b[38;5;241m=\u001b[39m model_sem\u001b[38;5;241m.\u001b[39msimilarity(dbg_embeds_sem, dbg_embeds_sem)\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:480\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    479\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 480\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_length(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[1;32m    481\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [sentences[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:480\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    479\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 480\u001b[0m length_sorted_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort([\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m sentences])\n\u001b[1;32m    481\u001b[0m sentences_sorted \u001b[38;5;241m=\u001b[39m [sentences[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m length_sorted_idx]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatches\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1264\u001b[0m, in \u001b[0;36mSentenceTransformer._text_length\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text)\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text])\n",
      "File \u001b[0;32m~/fastai/venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text)\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m text])\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float32' has no len()"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_sem = SentenceTransformer(\"sentence-transformers/stsb-xlm-r-multilingual\")\n",
    "\n",
    "print(\"SEMANTIC SIMILARITY\")\n",
    "print(SYNTH_METHOD)\n",
    "dbg_embeds_sem = model_sem.encode(dbg_txt)\n",
    "print(dbg_embeds_sem.shape)\n",
    "sim_sem = model_sem.similarity(dbg_embeds_sem, dbg_embeds_sem)\n",
    "sim_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['العلم اسم قام ه لا اسم اسم ه', 'و اسم ه اسم ه لا اسم ه نعت']\n",
      "عمر الفتى ذكره لا طول مدته <=> وموته خزيه لا يومه الآتي\n",
      "  meaning = 33.3%\n",
      "  syntax  = 87.1%\n",
      "['و ما اسم و اسم إلا اسم ه', 'و لا اسم اسم أن قام اسم']\n",
      "وما المال والأهلون إلا وديعة <=> ولا بد يوما أن ترد الودائع\n",
      "  meaning = 33.5%\n",
      "  syntax  = 88.3%\n",
      "['و ما اسم إلا ك العلم و اسم ه', 'و ما اسم و اسم إلا اسم ه']\n",
      "وما المرء إلا كالهلال وضوئه <=> وما المال والأهلون إلا وديعة\n",
      "  meaning = 58.6%\n",
      "  syntax  = 91.2%\n",
      "['قام اسم اسم نعت', 'قام اسم اسم نعت']\n",
      "أغرقت الأمطار أراضي واسعة <=> أصدرت الوزارة بيانا جديدا\n",
      "  meaning = 19.3%\n",
      "  syntax  = 100.0%\n",
      "['العلم اسم قام ه لا اسم اسم ه', 'أن قام قام اسم اسم من أن قام نعت']\n",
      "عمر الفتى ذكره لا طول مدته <=> أن يبقى ذكر الفتى خير من أن يعمر طويلا\n",
      "  meaning = 73.0%\n",
      "  syntax  = 71.0%\n",
      "['العلم اسم قام ه لا اسم اسم ه', 'قام اسم ما قام قام ه']\n",
      "عمر الفتى ذكره لا طول مدته <=> يعمر الفتى ما دام ذكره\n",
      "  meaning = 92.8%\n",
      "  syntax  = 76.9%\n",
      "['العلم اسم قام ه لا اسم اسم ه', 'قام العلم اسم قام ب اسم اسم لكن اسم']\n",
      "عمر الفتى ذكره لا طول مدته <=>  يطول عمر الفتى ليس بطول السنين لكن الذكر\n",
      "  meaning = 82.4%\n",
      "  syntax  = 88.9%\n"
     ]
    }
   ],
   "source": [
    "for i, j in [\n",
    "    ('عمر الفتى ذكره لا طول مدته', 'وموته خزيه لا يومه الآتي'),\n",
    "    ('وما المال والأهلون إلا وديعة', 'ولا بد يوما أن ترد الودائع'),\n",
    "    ('وما المرء إلا كالهلال وضوئه', 'وما المال والأهلون إلا وديعة'),\n",
    "    ('أغرقت الأمطار أراضي واسعة', 'أصدرت الوزارة بيانا جديدا'),\n",
    "    ('عمر الفتى ذكره لا طول مدته', 'أن يبقى ذكر الفتى خير من أن يعمر طويلا',),\n",
    "    ('عمر الفتى ذكره لا طول مدته', 'يعمر الفتى ما دام ذكره'),\n",
    "    ('عمر الفتى ذكره لا طول مدته', ' يطول عمر الفتى ليس بطول السنين لكن الذكر'),\n",
    "]:\n",
    "    enc_sem = model_sem.encode([i, j])\n",
    "    synth = list(map(synth_txt, [i, j]))\n",
    "    print(synth)\n",
    "    enc = model.encode(synth)\n",
    "    print(f\"{i} <=> {j}\")\n",
    "    print(f\"  meaning = {model_sem.similarity(enc_sem, enc_sem)[0][1] * 100:.1f}%\")\n",
    "    print(f\"  syntax  = {model.similarity(enc, enc)[0][1] * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arud+literal\n",
      "SYNTACTIC SIMILARITY\n",
      "(7, 768)\n",
      "عمر الفتى ذكره لا طول مدته\n",
      "العلم اسم قام ه لا اسم اسم ه\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4567",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(dbg_txt[i])\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(synth_txt(dbg_txt[i]))\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mclusters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     22\u001b[0m sim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msimilarity(dbg_embeds, dbg_embeds)\n",
      "\u001b[0;31mKeyError\u001b[0m: 4567"
     ]
    }
   ],
   "source": [
    "dbg_txt = [\n",
    "    'عمر الفتى ذكره لا طول مدته',\n",
    "    'وموته خزيه لا يومه الآتي',\n",
    "    'أن يبقى ذكر الفتى خير من أن يعمر طويلا',\n",
    "    'ولا بد يوما أن ترد الودائع',\n",
    "    'وما المرء إلا كالهلال وضوئه',\n",
    "    'وما المال والأهلون إلا وديعة',\n",
    "\"\"\"\n",
    "ليس كل ما يتمنى المرء يدركه\n",
    "\"\"\".strip(),\n",
    "]\n",
    "\n",
    "print(SYNTH_METHOD)\n",
    "print(\"SYNTACTIC SIMILARITY\")\n",
    "dbg_embeds = model.encode(list(map(synth_txt, dbg_txt)))\n",
    "print(dbg_embeds.shape)\n",
    "for i, label in enumerate(birch.predict(dbg_embeds)):\n",
    "    print(dbg_txt[i])\n",
    "    print(synth_txt(dbg_txt[i]))\n",
    "    print(f\"    {clusters[label][0]}\")\n",
    "    print()\n",
    "sim = model.similarity(dbg_embeds, dbg_embeds)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud+literal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['هَلْ غادَرَ الشُّعَرَاءُ مِنْ مُتَرَدَّمِ',\n",
       " 'أَم هَلْ عَرَفْتَ الدَّارَ بَعْدَ تَوَهُّمِ',\n",
       " 'يَا دَارَ عَبْلَةَ بِالجَوَاءِ تَكَلَّمِي',\n",
       " 'وَعِمِي صَبَاحاً دَارَ عَبْلَةَ وَاسْلَمِي',\n",
       " 'فَوَقَفْتُ فِيْهَا نَاقَتِي وَكَأَنَّهَا',\n",
       " 'فَدَنٌ لِأَقْضِيَ حَاجَةَ المُتَلَوِّمِ',\n",
       " 'وَتَحُلُّ عَبْلَةُ بِالجَوَاءِ وَأَهْلُنَا',\n",
       " 'بِالحَزْنِ فَالصَّمَّانِ فَالمُتَثَلَّمِ',\n",
       " 'حُيِّيْتَ مِنْ طَلَلٍ تَقَادَمَ عَهْدُهُ',\n",
       " 'أَقْوَى وَأَقْفَرَ بَعْدَ أُمِّ الهَيْثَمِ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(SYNTH_METHOD)\n",
    "\n",
    "corpus2 = []\n",
    "with open(f'antar.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if len(line) == 0: continue\n",
    "        corpus2.append(line)\n",
    "corpus2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ud+literal\n",
      "(148, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7254, 0.6753, 0.5923, 0.7133, 0.7120, 0.6892, 0.5891, 0.8856,\n",
       "         0.6990],\n",
       "        [0.7254, 1.0000, 0.6131, 0.5702, 0.6948, 0.7832, 0.6862, 0.6804, 0.6178,\n",
       "         0.8267],\n",
       "        [0.6753, 0.6131, 1.0000, 0.7099, 0.6706, 0.6661, 0.7284, 0.5277, 0.6431,\n",
       "         0.6898],\n",
       "        [0.5923, 0.5702, 0.7099, 1.0000, 0.7130, 0.6580, 0.8405, 0.5941, 0.6133,\n",
       "         0.7792],\n",
       "        [0.7133, 0.6948, 0.6706, 0.7130, 1.0000, 0.8320, 0.7885, 0.7173, 0.7099,\n",
       "         0.7781],\n",
       "        [0.7120, 0.7832, 0.6661, 0.6580, 0.8320, 1.0000, 0.7837, 0.7934, 0.6416,\n",
       "         0.7747],\n",
       "        [0.6892, 0.6862, 0.7284, 0.8405, 0.7885, 0.7837, 1.0000, 0.7406, 0.6735,\n",
       "         0.8553],\n",
       "        [0.5891, 0.6804, 0.5277, 0.5941, 0.7173, 0.7934, 0.7406, 1.0000, 0.5515,\n",
       "         0.6944],\n",
       "        [0.8856, 0.6178, 0.6431, 0.6133, 0.7099, 0.6416, 0.6735, 0.5515, 1.0000,\n",
       "         0.6905],\n",
       "        [0.6990, 0.8267, 0.6898, 0.7792, 0.7781, 0.7747, 0.8553, 0.6944, 0.6905,\n",
       "         1.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(SYNTH_METHOD)\n",
    "corpus2_txfm = list(map(synth_txt, corpus2))\n",
    "embeddings = model.encode(corpus2_txfm)\n",
    "print(embeddings.shape)\n",
    "model.similarity(embeddings[:10,:], embeddings[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "meanshift = MeanShift(bandwidth=2).fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_clustering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43msave_clustering\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeanshift\u001b[39m\u001b[38;5;124m\"\u001b[39m, meanshift\u001b[38;5;241m.\u001b[39mlabels_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_clustering' is not defined"
     ]
    }
   ],
   "source": [
    "clusters = save_clustering(\"meanshift\", meanshift.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
